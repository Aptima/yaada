{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to YAADA!","text":"YAADA Architecture <p>YAADA is a data architecture and analytics platform developed to support the full analytics development lifecycle, from prototyping in local Python to operational deployment as containerized microservices. YAADA\u2019s primary focus is ingesting, storing, and analyzing semi-structured document-oriented data and training, persisting, and applying analytic models. It leverages popular technologies such as Elasticsearch and Kibana for document storage and visualization and Jupyter for exploratory data analysis and analytic prototyping. It handles all the details of data management, analytic serving through REST and message-based APIs, while providing an analytic plugin API that allows analytic developers to focus on the algorithms. In addition, YAADA includes pre-built analytic wrappers for popular open source libraries for NLP and web scraping.</p>"},{"location":"api/","title":"YAADA Python API","text":""},{"location":"api/#analytic-context","title":"Analytic Context","text":""},{"location":"api/#yaada.core.analytic.context.AnalyticContext","title":"<code>AnalyticContext</code>","text":"Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>class AnalyticContext:\n    def __init__(\n        self,\n        analytic_name,\n        analytic_session_id,\n        parameters,\n        msg_service=None,\n        doc_service=None,\n        ob_service=None,\n        model_manager=None,\n        config=None,\n        schema_manager=None,\n        connect_to_services=True,\n        overrides={},\n    ):\n        self.config = config\n        if self.config is None:\n            if \"config\" in overrides:\n                self.config = YAADAConfig(overrides[\"config\"], overrides=overrides)\n            else:\n                self.config = YAADAConfig(overrides=overrides)\n        self.schema_manager = schema_manager\n        if self.schema_manager is None:\n            self.schema_manager = SchemaManager(self.config)\n\n        self.analytic_name = analytic_name\n        self.analytic_session_id = analytic_session_id\n        self.parameters = parameters\n        self.status = dict(\n            analytic_name=self.analytic_name,\n            analytic_session_id=self.analytic_session_id,\n            started=False,\n            finished=False,\n            input_stats={},\n            output_stats={},\n            error=False,\n            message=None,\n            parameters=self.parameters,\n        )\n        self._results = []\n        self._document_pipeline = None\n        self.last_flush = datetime.utcnow()\n        self._printer = None\n        self.overrides = overrides\n        self._start_time = datetime.utcnow()\n        self.msg_service = msg_service\n        self.doc_service = doc_service\n        self.ob_service = ob_service\n        self.model_manager = model_manager\n        self.results_in_status = False\n        self.migration = None\n        self._connect_to_services = connect_to_services\n\n        if self._connect_to_services:\n            if self.msg_service is None:\n                self.msg_service = make_message_service(\n                    self.config, overrides=overrides\n                )\n                self.msg_service.connect(\n                    f\"analytic-context-{analytic_name}-{analytic_session_id}\"\n                )\n\n            if self.doc_service is None:\n                self.doc_service = make_document_service(\n                    self.config, overrides=overrides\n                )\n\n            if self.ob_service is None:\n                self.ob_service = make_objectstorage_service(\n                    self.config, overrides=overrides\n                )\n\n            if self.model_manager is None:\n                self.model_manager = make_model_manager(self.config, self)\n\n            self.msg_service.set_analytic(analytic_name, analytic_session_id)\n\n        register_context_plugins(self)\n\n    def wait_for_ready(self, timeout=30):\n        start_time = time.time()\n        while True:\n            try:\n                self.document_counts()\n                return True\n            except Exception:\n                elapsed_time = time.time() - start_time\n                time.sleep(0.5)\n                if elapsed_time &gt;= timeout:\n                    raise TimeoutError(\"waited too long for analytic context\")\n\n    def set_analytic_name(self, analytic_name):\n\"\"\"\n        Assigns an ``analytic_name`` value of a context object.\n\n        Parameters:\n\n        * **analytic_name: str**\n\n          A description of the analytic's purpose\n\n        Example:\n\n        .. code-block:: python\n\n            context.set_analytic_name(\"jupyter\")\n        \"\"\"\n        self.analytic_name = analytic_name\n        # self.status['analytic_name'] = analytic_name\n\n    def set_analytic_session_id(self, analytic_session_id):\n\"\"\"\n        Assigns an ``analytic_session_id`` value of a context object.\n\n        Parameters:\n\n          * **analytic_session_id: str**\n\n            An identifier for this analytic object\n\n        Example:\n\n        .. code-block:: python\n\n            context.set_analytic_session_id(\"Query Data\")\n        \"\"\"\n        self.analytic_session_id = analytic_session_id\n        # self.status['analytic_session_id'] = analytic_session_id\n\n    def create_derived_context(self, analytic_name, analytic_session_id, parameters):\n        c = AnalyticContext(\n            analytic_name=analytic_name,\n            analytic_session_id=analytic_session_id,\n            parameters=parameters,\n            msg_service=self.msg_service,\n            doc_service=self.doc_service,\n            ob_service=self.ob_service,\n            model_manager=self.model_manager,\n            config=self.config,\n            schema_manager=self.schema_manager,\n            connect_to_services=self._connect_to_services,\n            overrides=self.overrides,\n        )\n        c.status[\"parent\"] = dict(\n            analytic_name=self.analytic_name,\n            analytic_session_id=self.analytic_session_id,\n        )\n        return c\n\n    def include_results_in_status(self, results_in_status):\n        self.results_in_status = results_in_status\n        self._results = []\n        # self.status['results'] = list()\n\n    def init_pipeline(self):\n        self._document_pipeline = make_pipeline(\n            self.create_derived_context(\"pipeline\", \"0\", {})\n        )\n\n    @property\n    def document_pipeline(self):\n        if self._document_pipeline is None:\n            analytic.find_and_register(self.config, pipelines=True)\n            self.init_pipeline()\n        return self._document_pipeline\n\n    def result_sync(\n        self,\n        docs,\n        process=True,\n        upsert=False,\n        archive=False,\n        barrier=False,\n        barrier_timeout=600,\n        raise_ingest_error=True,\n        validate=True,\n        refresh=False,\n    ):\n        if barrier:\n            sentinel_value = str(_uuid.uuid4())\n        else:\n            sentinel_value = None\n        for batch in utility.batched_generator(\n            apply_barrier_sentinel_generator(docs, \"_ingest_sentinel\", sentinel_value),\n            1000,\n        ):\n            # each batch is fully realized and has sentinel applied if necessary\n            doc_dict = {}\n            batch_to_store = []\n            for doc in batch:\n                doc = prepare_doc_for_insert(\n                    doc,\n                    self.analytic_name,\n                    self.analytic_session_id,\n                    upsert,\n                    archive=archive,\n                )\n                if process:\n                    doc = self.document_pipeline.process_document(doc)\n                if validate:\n                    try:\n                        self.schema_manager.validate_document(doc)\n                    except jsonschema.exceptions.ValidationError as e:\n                        self.doc_service.write_ingest_error(\n                            \"jsonschema\", dict(doc=doc, error=str(e))\n                        )\n                        if raise_ingest_error:\n                            raise e\n                        continue\n\n                batch_to_store.append(doc)\n                doc_dict[(doc[\"doc_type\"], doc[\"_id\"])] = doc\n                self._count_result(doc[\"doc_type\"])\n                if self.results_in_status:\n                    self._results.append(doc)\n            docs_flushed = self.doc_service.store_batch(\n                batch_to_store, raise_ingest_error=raise_ingest_error, refresh=refresh\n            )\n            for doc_type, _id in docs_flushed:\n                if (doc_type, _id) in doc_dict:\n                    self.msg_service.publish_sinklog(doc_dict[(doc_type, _id)])\n                else:\n                    logger.error(\n                        f\"{(doc_type,_id)} not found in doc_dict {doc_dict.keys()}\"\n                    )\n\n            if barrier:\n                for doc_type, _id in docs_flushed:\n                    self.ingest_barrier(\n                        doc_type,\n                        _id,\n                        barrier_timeout,\n                        \"_ingest_sentinel\",\n                        sentinel_value,\n                    )\n\n    def result_async(\n        self,\n        docs,\n        process=True,\n        upsert=False,\n        archive=False,\n        barrier=False,\n        barrier_timeout=600,\n        raise_ingest_error=True,\n        validate=True,\n    ):\n        docs_to_wait_for = []\n        if barrier:\n            sentinel_value = str(_uuid.uuid4())\n        else:\n            sentinel_value = None\n\n        for doc in apply_barrier_sentinel_generator(\n            docs, \"_ingest_sentinel\", sentinel_value\n        ):  # sentinel value is only applied if sentinel_value is non-None\n            doc = prepare_doc_for_insert(\n                doc, self.analytic_name, self.analytic_session_id, upsert\n            )\n            if validate:\n                self.schema_manager.validate_document(doc)\n                try:\n                    self.schema_manager.validate_document(doc)\n                except jsonschema.exceptions.ValidationError as e:\n                    self.doc_service.write_ingest_error(\n                        \"jsonschema\", dict(doc=doc, error=str(e))\n                    )\n                    if raise_ingest_error:\n                        raise e\n            if barrier:\n                docs_to_wait_for.append((doc[\"doc_type\"], doc[\"_id\"]))\n            if process:\n                self.msg_service.publish_ingest(doc)\n            else:\n                self.msg_service.publish_sink(doc)\n            self._count_result(doc[\"doc_type\"])\n            if self.results_in_status:\n                self._results.append(doc)\n\n        if barrier:\n            for doc_type, _id in docs_to_wait_for:\n                self.ingest_barrier(\n                    doc_type, _id, barrier_timeout, \"_ingest_sentinel\", sentinel_value\n                )\n\n    def result(\n        self,\n        docs,\n        process=True,\n        sync=True,\n        upsert=False,\n        archive=False,\n        barrier=False,\n        barrier_timeout=600,\n        raise_ingest_error=True,\n        validate=True,\n    ):\n        if docs is None:\n            return\n        if utility.isiterable(docs):\n            _docs = docs\n        else:\n            _docs = [docs]\n        if sync:\n            self.result_sync(\n                docs=_docs,\n                process=process,\n                upsert=upsert,\n                archive=archive,\n                raise_ingest_error=raise_ingest_error,\n                validate=validate,\n                refresh=barrier,\n            )\n        else:\n            self.result_async(\n                docs=_docs,\n                process=process,\n                upsert=upsert,\n                archive=archive,\n                barrier=barrier,\n                barrier_timeout=barrier_timeout,\n                raise_ingest_error=raise_ingest_error,\n                validate=validate,\n            )\n        self.report_status()\n\n    def update(\n        self,\n        doc,\n        process=True,\n        sync=True,\n        archive=False,\n        barrier=False,\n        barrier_timeout=600,\n        raise_ingest_error=True,\n        validate=True,\n    ):\n\"\"\"\n\n        Stores the document given by the ``doc`` parameter in Elasticsearch.\n\n          Parameters:\n\n          * **doc: dict, list[dict]**\n\n            The ``doc`` parameter, represents an Elasticsearch document to be ingested. The schema and details of a document can be found in :ref:`Documents`.\n\n            This parameter can be given as a dictionary representation of an Elasticsearch document, or a list of ``doc``'s which will result in a batch-ingest.\n\n          * **process: bool, default = True**\n\n            optional\n\n            If ``True``, the update process with run through the ingest pipeline\n\n          * **sync: bool, default = True**\n\n            optional\n\n            If value is set to ``True``, the process will be run synchronously, if set to ``False``, it will run asynchronously.\n\n          * **archive: bool, default = False**\n\n            optional\n\n            If the document or documents being ingested are from an archive, this parameter's value should be set\n            to ``True``. Otherwise, it will default to ``False``.\n\n            When ``False``, a document being ingested through the ``update`` function will be automtically set a timestamp value\n            to the field ``@timestamp`` to the same time that it was ingested. When ``True``, because the document is from an archive,\n            the ``@timestamp`` field will already exists and will not be reassigned. Therefore, it will keep it's value representing the time\n            when it was originally ingested.\n\n          * **barrier: bool, default=False**\n\n            optional\n\n            Setting ``barrier`` to ``True`` ensures that the document is indexed Elasticsearch\n            and searchable before the the update function finishes running.\n\n            Enabling ``barrier`` will dractically reduce throughput performace especially if ``update``\n            is being called repeatedly by updating one document at a time. The process will not be slowed\n            drastically if the ``update`` is run as a batch-ingest.\n\n          * **barrier_timeout: int, default=600**\n\n            optional\n\n            This value is the maximum amount of time in seconds that the function will wait\n            for the document to appear in Elasticsearch before finishing. If this maximum value\n            is reached and the document is not in elastic search, it will return an\n            ``IngestBarrierTimeout`` error. This only applies if the ``barrier`` parameter is set\n            to ``True``.\n\n          * **raise_ingest_error: bool, default=True**\n\n            optional\n\n\n        Short example:\n\n        .. code-block:: python\n\n          doc = {\n              _id = \"123\",\n              doc_type = \"EmployeeSurvey\",\n              favorite_color = \"red\",\n              favoite_number = 4\n          }\n          context.update(doc)\n\n\n        \"\"\"\n        self.result(\n            doc,\n            process=process,\n            sync=sync,\n            upsert=True,\n            archive=archive,\n            barrier=barrier,\n            barrier_timeout=barrier_timeout,\n            raise_ingest_error=raise_ingest_error,\n            validate=validate,\n        )\n\n    def _count_result(self, doc_type):\n        if doc_type not in self.status[\"output_stats\"]:\n            self.status[\"output_stats\"][doc_type] = 0\n        self.status[\"output_stats\"][doc_type] += 1\n\n    def _count_input(self, doc_type):\n        if doc_type not in self.status[\"input_stats\"]:\n            self.status[\"input_stats\"][doc_type] = 0\n        self.status[\"input_stats\"][doc_type] += 1\n\n    def report_status(self, write_es=False, message=None):\n\"\"\"\n        Called automatically when analytics are started and finishing. Available to be called by Anaytic developers\n        to show intermediate status of a an analytic. To do this, call it in the ``run`` function of an Analytic.\n\n          Parameters:\n\n          * **write_es:bool, default=False**\n\n\n\n          * **message:str, default=None**\n\n            A string value containing the message to display.\n\n        \"\"\"\n        self.status[\"@timestamp\"] = datetime.utcnow()\n        self.status[\"analytic_compute_duration_seconds\"] = (\n            datetime.utcnow() - self._start_time\n        ).total_seconds()\n        if message is not None:\n            self.status[\"message\"] = message\n            if self._printer is not None:\n                self._printer(message)\n        self.msg_service.publish_analytic_status(\n            self.analytic_name, self.analytic_session_id, self.status\n        )\n        if write_es:\n            self.doc_service.write_analytic_status(\n                self.analytic_name, self.analytic_session_id, self.status\n            )\n\n    def _started(self):\n        self._start_time = datetime.utcnow()\n        self.status[\"input_stats\"] = {}\n        self.status[\"output_stats\"] = {}\n        self.status[\"error\"] = False\n        self.status[\"message\"] = None\n        self.status[\"analytic_finish_time\"] = None\n        self.status[\"finished\"] = False\n        self.status[\"analytic_compute_duration_seconds\"] = None\n        self.status[\"started\"] = True\n        self.status[\"analytic_start_time\"] = self._start_time.isoformat()\n        self.report_status(write_es=True)\n\n    def _finished(self):\n        self._finish_time = datetime.utcnow()\n        self.status[\"analytic_finish_time\"] = self._finish_time.isoformat()\n        self.status[\"finished\"] = True\n        self.status[\"analytic_compute_duration_seconds\"] = (\n            self._finish_time - self._start_time\n        ).total_seconds()\n        self.report_status(write_es=True)\n\n    def error(self, message):\n        self.status[\"error\"] = True\n        self.status[\"message\"] = message\n        self.report_status(write_es=True)\n\n    def query(\n        self,\n        doc_type,\n        query={\"query\": {\"match_all\": {}}},\n        size=None,\n        scroll_size=1000,\n        scroll=\"2m\",\n        raw=False,\n        source=None,\n    ):\n\"\"\"\n        Returns results of a query to Elasticsearch as a generator object. It uses an abstraction of the `scroll API &lt;https://www.elastic.co/guide/en/elasticsearch/reference/7.9/scroll-api.html&gt;`_.\n\n        Parameters:\n\n          * **doc_type: str**\n\n            This represents an Elasticsearch index. The query will search the index.\n\n            Every document in Elasticsearch has a required ``doc_type`` field as described in the :ref:`Documents` section.\n\n          * **query: dict, default={\"query\": {\"match_all\": {}}}**\n\n            optional\n\n            An Elasticsearch query. More info can be found in `Elasticsearch documentation.\n            &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html&gt;`_\n          * **size:int, default=None**\n\n            optional\n\n            Specifies number of documents to be returned. If the value is greater than the\n            resulting number of documents, the resulting documents will all be returned.\n          * **scroll_size:int, defaults=1000**\n\n            optional\n\n          * **scroll:str, default='2m'**\n\n            optional\n\n          * **raw:bool, default=False**\n\n            optional\n\n          * **source:list, default=None**\n\n            optional\n\n            A list of fields that will be shown for each document returned by the query. Used to limit the fields that each document will return.\n            Must include ``_id`` and ``doc_type``.\n\n        \"\"\"\n\n        if query is None:\n            query = {\"query\": {\"match_all\": {}}}\n        if source is not None:\n            query[\"_source\"] = source\n        for d in self.doc_service.query(\n            doc_type, query, size=size, scroll_size=scroll_size, scroll=scroll, raw=raw\n        ):\n            if not raw:\n                self._count_input(d[\"doc_type\"])\n            yield d\n        self.report_status()\n\n    def paged_query(\n        self,\n        doc_type,\n        query={\"query\": {\"match_all\": {}}},\n        page_from=0,\n        page_size=10,\n        source=None,\n        source_exclude=None,\n        source_include=None,\n        raw=False,\n    ):\n        return self.doc_service.paged_query(\n            doc_type=doc_type,\n            query=query,\n            page_from=page_from,\n            page_size=page_size,\n            _source=source,\n            _source_exclude=source_exclude,\n            _source_include=source_include,\n            raw=raw,\n        )\n\n    def query_count(self, doc_type, query={\"query\": {\"match_all\": {}}}):\n        if query is None:\n            query = {\"query\": {\"match_all\": {}}}\n\n        return self.doc_service.query_count(doc_type, query)\n\n    def get(\n        self,\n        doc_type,\n        id,\n        source=None,\n        source_exclude=None,\n        source_include=None,\n        wait=False,\n        timeout=60,\n        sentinel_key=\"_ingest_sentinel\",\n        sentinel_value=None,\n    ):\n\"\"\"\n        Returns a document from Elasticsearch.\n\n        Parameters:\n\n          * **doc_type: str**\n\n            The document type of the desired document.\n\n          * **id: str**\n\n            The identifier of a single document.\n\n          * **source: list, defult=None**\n\n            optional\n\n            A list containing the fields that will be returned for the document. Must include ``_id`` and ``doc_type``.\n\n          * **source_exclude:list, default=None**\n\n            optional\n\n            A list containing the fields that will be excluded from what is returned for the document.\n\n          * **source_include:list, default=None**\n\n            optional\n\n            A list containing the fields that will returned for the document.\n\n          * **wait:bool, default=False**\n\n            optional\n\n\n            If True, this function will wait to return until the document is searchable in Elasticsearch.\n\n          * **timeout:int, default=60**\n\n            optional\n\n            This specifies the maximum time to wait for the document to be searchable in Elasticsearch if the ``wait`` parameter is set to ``True``.\n\n          * **sentinel_key:str, default='_ingest_sentinel'**\n\n            optional\n\n            Advance Usage\n\n            Elasticsearch field that contains the ``sentinel_value``. By default, there will be an ``_ingest_sentinal`` Elasticsearch field.\n\n\n            This parameter relates to the ``barrier`` concept used as a parameter in the :py:meth:`yaada.analytic.context.AnalyticContext.update` method.\n            If this ``get`` method is being used to retrieve a document in Elasticsearch soon after it has been ingested, this can be used to wait until\n            that value is searchable in Elasticsearch before it is retrieved. This parameter will only be applied if the ``wait`` parameter\n            is set to ``True``.\n\n          * **sentinel_value:str, default=None**\n\n            optional\n\n            Advance Usage\n\n            This parameter is used to ensure that an updated document has fully updated in Elasticsearch before it is retrieved by the ``get`` method. Because a\n            document that is being updated may already exist, YAADA needs a way to know that it has been updated before retrieving the document.\n            This is done by introducing a new value to the document, called a sentinal value. If a value is specified for this parameter, YAADA will\n            check that this value matches up with the value in the ``sentinel_key`` field for the document before retrieving it, to know that it has been updated.\n\n        .. code-block:: python\n\n          context.get(\"Publication\", \"2cf0d07c-cba9-47e3-9063-5d6265e26089\")\n\n        \"\"\"\n        if wait:\n            self.ingest_barrier(\n                doc_type,\n                id,\n                timeout=timeout,\n                sentinel_key=sentinel_key,\n                sentinel_value=sentinel_value,\n            )\n        return self.doc_service.get(\n            doc_type,\n            id,\n            _source=source,\n            _source_exclude=source_exclude,\n            _source_include=source_include,\n        )\n\n    def exists(self, doc_type, id):\n\"\"\"\n        Returns a bool value describing whether or not a document exists.\n\n        Parameters:\n\n          * **doc_type: str**\n\n              The document type.\n          * **id: str**\n\n              The unique id of the document\n\n\n        Example\n\n        .. code-block:: python\n\n          context.exists(\"Publication\", \"f33bca95-3caa-46ac-8667-448ba4973190\")\n        \"\"\"\n        return self.doc_service.exists(doc_type, id)\n\n    def mget(\n        self, doc_type, ids, source=None, source_exclude=None, source_include=None\n    ):\n\"\"\"\n        Returns multiple documents from Elasticsearch.\n\n        Parameters:\n\n          * **doc_type: str**\n\n            The document type of the documents to be returned.\n\n          * **ids: list**\n\n            List of unique identifiers of documents in Elasticsearch\n\n          * **source: list, defult=None**\n\n            optional\n\n            A list containing the fields that will be returned for the document.\n\n          * **source_exclude, default=None**\n\n            optional\n\n            A list containing the fields that will be excluded from what is returned for the document.\n\n          * **source_include, default=None**\n\n            optional\n\n            A list containing the fields that will returned for the document.\n\n\n        Example:\n\n        .. code-block:: python\n\n          context.mget(\"Publication\", [\"2cf0d07c-cba9-47e3-9063-5d6265e26089\", \"b0e062ac-0e84-40db-8ecd-36e1aa0e264b\"])\n        \"\"\"\n        return self.doc_service.mget(\n            doc_type,\n            ids,\n            _source=source,\n            _source_exclude=source_exclude,\n            _source_include=source_include,\n        )\n\n    def delete_by_query(self, doc_type, query={\"query\": {\"match_all\": {}}}):\n\"\"\"\n        Delete all documents that result from the a given query.\n\n        Parameters:\n\n          * **doc_type: str**\n\n            The document type.\n\n          * **query: dict**\n\n            And Elasticsearch query. Elasticsearch's documentation on querying can be found `here\n            &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html&gt;`_.\n\n        Example:\n\n        .. code-block:: python\n\n          context.delete_by_query(\"Publication\", {\n              'query': {\n                  'match': {\n                      '_id': '2cf0d07c-cba9-47e3-9063-5d6265e26089'\n                  }\n              }\n          })\n        \"\"\"\n        if query is None:\n            query = {\"query\": {\"match_all\": {}}}\n        self.doc_service.delete_by_query(doc_type, query)\n\n    def delete(self, doc_type, id):\n\"\"\"\n        Delete a single document in Elasticsearch.\n\n        Parameters:\n\n          * **doc_type: str**\n\n            The document type of the documents to be returned.\n\n          * **id: str**\n\n            Identifier of document in Elasticsearch.\n\n        Example:\n\n        .. code-block:: python\n\n            context.delete(\"Publication\", \"b0e062ac-0e84-40db-8ecd-36e1aa0e264b\")\n\n        \"\"\"\n        self.doc_service.delete(doc_type, id)\n\n    def delete_index(self, doc_type, initialize_index=True):\n\"\"\"\n        Deletes an index in Elasticsearch.\n\n        Parameters:\n\n          * **doc_type: str**\n\n            Documents with this ``doc_type`` will all be deleted.\n\n        Example:\n\n        .. code-block:: python\n\n          context.delete_index(\"Publication\")\n        \"\"\"\n        self.doc_service.delete_index(doc_type, initialize_index=initialize_index)\n\n    def save_model_instance(self, model):\n        modelservice.save_model_instance(self.ob_service, model)\n        self.status[\"model_name\"] = model.model_name\n        self.status[\"model_instance_id\"] = model.model_instance_id\n\n    def load_model_instance(self, model_name, model_instance_id):\n        model = modelservice.load_model_instance(\n            self.ob_service, model_name, model_instance_id\n        )\n        self.status[\"model_name\"] = model.model_name\n        self.status[\"model_instance_id\"] = model.model_instance_id\n        return model\n\n    def sentinel_present(self, doc_type, id, sentinel_key, sentinel_value):\n\"\"\"check if a document is searchable with a given sentinel value.\"\"\"\n        docs = list(\n            self.query(\n                doc_type,\n                {\n                    \"query\": {\n                        \"bool\": {\n                            \"must\": [\n                                {\"term\": {\"_id\": id}},\n                                {\"term\": {f\"{sentinel_key}.keyword\": sentinel_value}},\n                            ]\n                        }\n                    }\n                },\n            )\n        )\n        if len(docs) &gt; 0:\n            doc = docs[0]\n            if sentinel_key in doc:\n                return doc[sentinel_key] == sentinel_value\n\n        return False\n\n    def ingest_barrier(\n        self,\n        doc_type,\n        id,\n        timeout=60,\n        sentinel_key=\"_ingest_sentinel\",\n        sentinel_value=None,\n    ):\n\"\"\"\n        Describe sentinal key and value here so that it can be refered to.\n\n        Args:\n            doc_type ([type]): [description]\n            id ([type]): [description]\n            timeout (int, optional): [description]. Defaults to 60.\n            sentinel_key (str, optional): [description]. Defaults to '_ingest_sentinel'.\n            sentinel_value ([type], optional): [description]. Defaults to None.\n\n        Raises:\n            IngestBarrierTimeout: [description]\n        \"\"\"\n        delays = [0, 1, 5, 10, 30]\n        delay = 0\n        found = False\n        start_time = time.time()\n        while not found:\n            elapsed = time.time() - start_time\n            if timeout is not None and elapsed &gt;= timeout:\n                raise IngestBarrierTimeout(\n                    f\"timeout of {timeout} reached and {doc_type}:{id} not found in total of {elapsed} seconds\"\n                )\n\n            if delays:\n                delay = delays.pop(0)\n\n            if timeout is not None and timeout - elapsed &lt; delay:\n                delay = timeout - elapsed\n            if delay &gt; 0:\n                # print(f\"trying in {delay} seconds\")\n                time.sleep(delay)\n            if sentinel_value is None:\n                found = self.exists(doc_type, id)\n            else:\n                found = self.exists(doc_type, id) and self.sentinel_present(\n                    doc_type, id, sentinel_key, sentinel_value\n                )\n\n    def invalidate_model_instance(self, model_name, model_instance_id):\n        modelservice.invalidate_model_instance(model_name, model_instance_id)\n\n    def document_counts(self):\n\"\"\"\n        Returns an object with a breakdown of each index and the count of documents\n        in each index. The keys are each ``doc_type``'s in Elasticsearch and the values are\n        the number of documents currently stored.\n\n        Example:\n\n        .. code-block:: python\n\n            context.document_counts()\n        \"\"\"\n        return self.doc_service.document_counts()\n\n    def term_counts(self, doc_type, term, query={\"query\": {\"match_all\": {}}}):\n\"\"\"\n\n        Returns an object describing the frequency of terms for a given field of an index by running an Elasticsearch terms aggregation.\n\n        Parameters:\n\n          * **doc_type: str**\n\n            The document type, this represents an Elasticsearch index.\n\n          * **term: str**\n\n            The field that will be inspected.\n\n          * **query: dict, default={\"query\":{\"match_all\":{}}}**\n\n            optional\n\n            Provides a filter for the documents that will be aggregated.\n\n        \"\"\"\n        return self.doc_service.term_counts(doc_type, term, query)\n\n    def document_mappings(self, doc_type, *fields):\n\"\"\"\n        This is a depricated method, use :py:meth:`yaada.analytic.context.AnalyticContext.index_field_mappings` instead.\n        \"\"\"\n        return self.doc_service.index_field_mappings(doc_type, *fields)\n\n    def index_field_mappings(self, doc_type, *fields):\n        # Link out specific Elasticsearch call\n\"\"\"\n        Returns the Elasticsearch mapping for the given ``doc_type``. The Elasticsearch documentation\n        has a section on `mappings. &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html&gt;`_\n\n        Parameters:\n\n          * **doc_type: str**\n\n            The document type, this represents an Elasticsearch index.\n\n          * **fields: str **\n\n            optional\n\n            Limits the mapping to fields given for this parameter. Wildcards can be used.\n        \"\"\"\n        return self.doc_service.index_field_mappings(doc_type, *fields)\n\n    def index_mappings(self, doc_type):\n\"\"\"\n        Returns raw index mapping for an Elasticsearch index. Includes Elasticsearch metadata that :py:meth:`yaada.analytic.context.AnalyticContext.index_field_mappings` does not.\n\n        Parameters:\n\n          * **doc_type: str**\n\n            The document type, this represents an Elasticsearch index.\n\n        \"\"\"\n        return self.doc_service.index_mappings(doc_type)\n\n    def index_settings(self, doc_type):\n\"\"\"\n        Returns an object with an index's Elasticsearch settings. `Documentation on Elasticsearch's settings &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html#index-modules-settings&gt;`_\n\n        Parameters:\n\n          * **doc_type: str**\n\n            The document type, this represents an Elasticsearch index.\n\n\n        \"\"\"\n        return self.doc_service.index_settings(doc_type)\n\n    def init_index(self, doc_type, settings={}, mappings={}, aliases={}):\n        self.doc_service.init_index(\n            doc_type, settings=settings, mappings=mappings, aliases=aliases\n        )\n\n    def aggregation(self, doc_type, query):\n\"\"\"\n        Depricated, use :py:meth:`yaada.analytic.context.AnalyticContext.rawquery` instead.\n        \"\"\"\n        return self.doc_service.rawquery(doc_type, query)\n\n    def rawquery(self, doc_type, query):\n\"\"\"\n        Returns results to a query.\n\n        This returns the bare elasticsearch results. Use this to access to the scoring information or aggregations.\n\n        Parameters:\n\n          * **doc_type: str**\n\n            The document type, this represents an Elasticsearch index.\n\n          * **query: dict**\n\n            Elasticsearch query, more documentation can be found `here &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html&gt;`_.\n\n        \"\"\"\n        return self.doc_service.rawquery(doc_type, query)\n\n    def document_type_exists(self, doc_type):\n\"\"\"\n        Returns a bool value describing whether a ``doc_type``\n        exists.\n\n        Parameters:\n\n          * **doc_type:str**\n\n            The document type, this represents an Elasticsearch index.\n\n        Example:\n\n         .. code-block:: python\n\n            context.document_type_exists(\"Publication\")\n        \"\"\"\n        return self.doc_service.document_type_exists(doc_type)\n\n    def query_hit_count(self, doc_type, query):\n        q = query.copy()\n        q[\"size\"] = 0\n        return (\n            self.doc_service.aggregation(doc_type, q).get(\"hits\", {}).get(\"total\", None)\n        )\n\n    def sync_exec_analytic(\n        self,\n        analytic_name,\n        analytic_session_id=None,\n        parameters={},\n        include_results=False,\n        printer=print,\n    ):\n\"\"\"\n        Synchronously run an analytic. A tutorial on writing and running yaada analytics can be found in the :ref:`Writing an Analytic&lt;Writing an Analytic&gt;` section.\n\n        Parameters:\n\n          * **analytic_name: str**\n\n            The full analytic class name.\n\n          * **analytic_session_id: str, default=None**\n\n            optional\n\n            This value will be stored in Elasticsearch to mark the documents changed by this analytic session.\n            If a value is not defined, the context object's ``analytic_session_id`` value will be automatically assigned.\n\n          * **parameters: dict, default={}**\n\n            optional\n\n            This value corresponds to the parameters of the yaada analytic being run.\n\n            The parameters are assigned by setting each key in this dict to a string\n            of the name of a parameter. The parameter will be assinged to the value given in the key-value pair.\n\n          * **include_results: bool, default=False**\n\n            If running an analytic synchronously, setting this to  ``True`` will return the produced documents from the analytic.\n\n            Warning - Will Produce a Large Output\n\n            optional\n\n          * **printer: function, default=print**\n\n            optional\n\n            Advanced Usage, Experimental API subject to change, not recommended for users\n\n        Example:\n\n        .. code-block:: python\n\n          context.sync_exec_analytic(\n              analytic_name=\"yaada.analytic.builtin.newspaper.NewspaperScrapeSources\",\n              parameters={\"content\":True,\"scrape_top_image\":False,\"memoize\":True})\n\n        \"\"\"\n        if analytic_session_id is None:\n            analytic_session_id = str(_uuid.uuid4())\n        c = self.create_derived_context(analytic_name, analytic_session_id, parameters)\n        return sync_exec_analytic(\n            c,\n            analytic_name,\n            analytic_session_id,\n            parameters,\n            include_results=include_results,\n            printer=printer,\n        )\n\n    def async_exec_analytic(\n        self,\n        analytic_name,\n        analytic_session_id=None,\n        parameters={},\n        worker=\"default\",\n        image=None,\n        gpu=False,\n        login=None,\n        watch=False,\n        stream_status=False,\n    ):\n\"\"\"\n        Asynchronously run an analytic.\n\n        Parameters:\n\n          * **analytic_name: str**\n\n            The full analytic class name.\n\n          * **analytic_session_id: str, default=None**\n\n            optional\n\n            This value will be stored in Elasticsearch to mark the documents changed by this analytic session.\n            If a value is not defined, the context object's ``analytic_session_id`` value will be automatically assigned.\n\n          * **parameters: dict, default={}**\n\n            optional\n\n            This value corresponds to the parameters of the yaada analytic being run.\n\n            The parameters are assigned by setting each key in this dict to a string\n            of the name of a parameter. The parameter will be assinged to the value given in the key-value pair.\n\n          * **worker:str, default='default**\n\n            optional\n\n            Use for selecting which analytic worker to use for execution\n\n            Advanced Usage, Experimental API subject to change, not recommended for users\n\n          * **watch:bool, default=False**\n\n            optional\n\n            Advanced Usage, Experimental API subject to change, not recommended for users\n\n        \"\"\"\n        if analytic_session_id is None:\n            analytic_session_id = str(_uuid.uuid4())\n        if stream_status:\n            self.msg_service.subscribe_analytic_status(\n                analytic_name, analytic_session_id\n            )\n        status = async_exec_analytic(\n            self.msg_service,\n            analytic_name,\n            analytic_session_id,\n            parameters,\n            worker=worker,\n            image=image,\n            gpu=gpu,\n            login=login,\n        )\n\n        def gen(context, analytic_name, analytic_session_id):\n            while True:\n                r = context.msg_service.fetch()\n                for doc in r:\n                    yield doc\n                    if doc.get(\"finished\", False) or doc.get(\"error\", False):\n                        context.msg_service.unsubscribe_analytic_status(\n                            analytic_name, analytic_session_id\n                        )\n                        context.msg_service.fetch()\n                        return\n\n        if stream_status:\n            return gen(self, analytic_name, analytic_session_id)\n        else:\n            return status\n\n    def fetch_file_to_temp(self, remote_file_path):\n        return self.ob_service.fetch_file_to_temp(remote_file_path)\n\n    def fetch_file_to_directory(self, remote_file_path, local_dir, filename):\n        return self.ob_service.fetch_file_to_directory(\n            remote_file_path, local_dir, filename\n        )\n\n    def fetch_artifact_to_directory(\n        self, doc, artifact_type, cache_dir=\"/tmp/yaada/artifacts-cache\"\n    ):\n        return self.ob_service.fetch_artifact_to_directory(\n            doc, artifact_type, cache_dir\n        )\n\n    def save_artifact(self, doc, artifact_type, filename, file):\n        return self.ob_service.save_artifact(doc, artifact_type, filename, file)\n\n    def save_artifact_dir(self, doc, artifact_type, local_dir, re_skip_matches=None):\n        return self.ob_service.save_artifact_dir(\n            doc, artifact_type, local_dir, re_skip_matches\n        )\n\n    def finalize(self):\n        self.msg_service.disconnect()\n\n    def get_external_mqtt_service(self):\n        return make_external_mqtt_service(self.config)\n\n    def get_model_manager(self):\n        return self.model_manager\n\n    def analytic_description(self, name=None):\n        if name is None:\n            return analytic.get_analytics()\n        else:\n            return analytic.analytic_description(name)\n\n    def publish_event(self, topic, data):\n        self.msg_service.publish_event(topic, data)\n\n    def subscribe_event(self, sub, dest=None):\n        def remove_topic_prefix(doc):\n            newdoc = dict(**doc)\n            prefix = f\"{self.msg_service._event_topic_base}/\"\n            if newdoc[\"_topic\"].startswith(prefix):\n                newdoc[\"_topic\"] = newdoc[\"_topic\"][len(prefix) :]\n            return newdoc\n\n        if dest is None:\n            s = BufferedStream(transform=remove_topic_prefix)\n        else:\n            s = dest\n        self.msg_service.subscribe_event(sub, dest=s)\n        return s\n\n    def unsubscribe_event(self, sub):\n        self.msg_service.unsubscribe_event(sub)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.aggregation","title":"<code>aggregation(doc_type, query)</code>","text":"<p>Depricated, use :py:meth:<code>yaada.analytic.context.AnalyticContext.rawquery</code> instead.</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def aggregation(self, doc_type, query):\n\"\"\"\n    Depricated, use :py:meth:`yaada.analytic.context.AnalyticContext.rawquery` instead.\n    \"\"\"\n    return self.doc_service.rawquery(doc_type, query)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.async_exec_analytic","title":"<code>async_exec_analytic(analytic_name, analytic_session_id=None, parameters={}, worker='default', image=None, gpu=False, login=None, watch=False, stream_status=False)</code>","text":"<p>Asynchronously run an analytic.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**analytic_name</code> <p>str**</p> <p>The full analytic class name.</p> required <code>*</code> <code>**analytic_session_id</code> <p>str, default=None**</p> <p>optional</p> <p>This value will be stored in Elasticsearch to mark the documents changed by this analytic session. If a value is not defined, the context object's <code>analytic_session_id</code> value will be automatically assigned.</p> required <code>*</code> <code>**parameters</code> <p>dict, default={}**</p> <p>optional</p> <p>This value corresponds to the parameters of the yaada analytic being run.</p> <p>The parameters are assigned by setting each key in this dict to a string of the name of a parameter. The parameter will be assinged to the value given in the key-value pair.</p> required <code>*</code> <code>**worker</code> <p>str, default='default**</p> <p>optional</p> <p>Use for selecting which analytic worker to use for execution</p> <p>Advanced Usage, Experimental API subject to change, not recommended for users</p> required <code>*</code> <code>**watch</code> <p>bool, default=False**</p> <p>optional</p> <p>Advanced Usage, Experimental API subject to change, not recommended for users</p> required Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def async_exec_analytic(\n    self,\n    analytic_name,\n    analytic_session_id=None,\n    parameters={},\n    worker=\"default\",\n    image=None,\n    gpu=False,\n    login=None,\n    watch=False,\n    stream_status=False,\n):\n\"\"\"\n    Asynchronously run an analytic.\n\n    Parameters:\n\n      * **analytic_name: str**\n\n        The full analytic class name.\n\n      * **analytic_session_id: str, default=None**\n\n        optional\n\n        This value will be stored in Elasticsearch to mark the documents changed by this analytic session.\n        If a value is not defined, the context object's ``analytic_session_id`` value will be automatically assigned.\n\n      * **parameters: dict, default={}**\n\n        optional\n\n        This value corresponds to the parameters of the yaada analytic being run.\n\n        The parameters are assigned by setting each key in this dict to a string\n        of the name of a parameter. The parameter will be assinged to the value given in the key-value pair.\n\n      * **worker:str, default='default**\n\n        optional\n\n        Use for selecting which analytic worker to use for execution\n\n        Advanced Usage, Experimental API subject to change, not recommended for users\n\n      * **watch:bool, default=False**\n\n        optional\n\n        Advanced Usage, Experimental API subject to change, not recommended for users\n\n    \"\"\"\n    if analytic_session_id is None:\n        analytic_session_id = str(_uuid.uuid4())\n    if stream_status:\n        self.msg_service.subscribe_analytic_status(\n            analytic_name, analytic_session_id\n        )\n    status = async_exec_analytic(\n        self.msg_service,\n        analytic_name,\n        analytic_session_id,\n        parameters,\n        worker=worker,\n        image=image,\n        gpu=gpu,\n        login=login,\n    )\n\n    def gen(context, analytic_name, analytic_session_id):\n        while True:\n            r = context.msg_service.fetch()\n            for doc in r:\n                yield doc\n                if doc.get(\"finished\", False) or doc.get(\"error\", False):\n                    context.msg_service.unsubscribe_analytic_status(\n                        analytic_name, analytic_session_id\n                    )\n                    context.msg_service.fetch()\n                    return\n\n    if stream_status:\n        return gen(self, analytic_name, analytic_session_id)\n    else:\n        return status\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.delete","title":"<code>delete(doc_type, id)</code>","text":"<p>Delete a single document in Elasticsearch.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>The document type of the documents to be returned.</p> required <code>*</code> <code>**id</code> <p>str**</p> <p>Identifier of document in Elasticsearch.</p> required <p>Example:</p> <p>.. code-block:: python</p> <pre><code>context.delete(\"Publication\", \"b0e062ac-0e84-40db-8ecd-36e1aa0e264b\")\n</code></pre> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def delete(self, doc_type, id):\n\"\"\"\n    Delete a single document in Elasticsearch.\n\n    Parameters:\n\n      * **doc_type: str**\n\n        The document type of the documents to be returned.\n\n      * **id: str**\n\n        Identifier of document in Elasticsearch.\n\n    Example:\n\n    .. code-block:: python\n\n        context.delete(\"Publication\", \"b0e062ac-0e84-40db-8ecd-36e1aa0e264b\")\n\n    \"\"\"\n    self.doc_service.delete(doc_type, id)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.delete_by_query","title":"<code>delete_by_query(doc_type, query={'query': {'match_all': {}}})</code>","text":"<p>Delete all documents that result from the a given query.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>The document type.</p> required <code>*</code> <code>**query</code> <p>dict**</p> <p>And Elasticsearch query. Elasticsearch's documentation on querying can be found <code>here &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html&gt;</code>_.</p> required <p>Example:</p> <p>.. code-block:: python</p> <p>context.delete_by_query(\"Publication\", {       'query': {           'match': {               '_id': '2cf0d07c-cba9-47e3-9063-5d6265e26089'           }       }   })</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def delete_by_query(self, doc_type, query={\"query\": {\"match_all\": {}}}):\n\"\"\"\n    Delete all documents that result from the a given query.\n\n    Parameters:\n\n      * **doc_type: str**\n\n        The document type.\n\n      * **query: dict**\n\n        And Elasticsearch query. Elasticsearch's documentation on querying can be found `here\n        &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html&gt;`_.\n\n    Example:\n\n    .. code-block:: python\n\n      context.delete_by_query(\"Publication\", {\n          'query': {\n              'match': {\n                  '_id': '2cf0d07c-cba9-47e3-9063-5d6265e26089'\n              }\n          }\n      })\n    \"\"\"\n    if query is None:\n        query = {\"query\": {\"match_all\": {}}}\n    self.doc_service.delete_by_query(doc_type, query)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.delete_index","title":"<code>delete_index(doc_type, initialize_index=True)</code>","text":"<p>Deletes an index in Elasticsearch.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>Documents with this <code>doc_type</code> will all be deleted.</p> required <p>Example:</p> <p>.. code-block:: python</p> <p>context.delete_index(\"Publication\")</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def delete_index(self, doc_type, initialize_index=True):\n\"\"\"\n    Deletes an index in Elasticsearch.\n\n    Parameters:\n\n      * **doc_type: str**\n\n        Documents with this ``doc_type`` will all be deleted.\n\n    Example:\n\n    .. code-block:: python\n\n      context.delete_index(\"Publication\")\n    \"\"\"\n    self.doc_service.delete_index(doc_type, initialize_index=initialize_index)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.document_counts","title":"<code>document_counts()</code>","text":"<p>Returns an object with a breakdown of each index and the count of documents in each index. The keys are each <code>doc_type</code>'s in Elasticsearch and the values are the number of documents currently stored.</p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code>context.document_counts()\n</code></pre> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def document_counts(self):\n\"\"\"\n    Returns an object with a breakdown of each index and the count of documents\n    in each index. The keys are each ``doc_type``'s in Elasticsearch and the values are\n    the number of documents currently stored.\n\n    Example:\n\n    .. code-block:: python\n\n        context.document_counts()\n    \"\"\"\n    return self.doc_service.document_counts()\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.document_mappings","title":"<code>document_mappings(doc_type, *fields)</code>","text":"<p>This is a depricated method, use :py:meth:<code>yaada.analytic.context.AnalyticContext.index_field_mappings</code> instead.</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def document_mappings(self, doc_type, *fields):\n\"\"\"\n    This is a depricated method, use :py:meth:`yaada.analytic.context.AnalyticContext.index_field_mappings` instead.\n    \"\"\"\n    return self.doc_service.index_field_mappings(doc_type, *fields)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.document_type_exists","title":"<code>document_type_exists(doc_type)</code>","text":"<p>Returns a bool value describing whether a <code>doc_type</code> exists.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>The document type, this represents an Elasticsearch index.</p> required Example <p>.. code-block:: python</p> <p>context.document_type_exists(\"Publication\")</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def document_type_exists(self, doc_type):\n\"\"\"\n    Returns a bool value describing whether a ``doc_type``\n    exists.\n\n    Parameters:\n\n      * **doc_type:str**\n\n        The document type, this represents an Elasticsearch index.\n\n    Example:\n\n     .. code-block:: python\n\n        context.document_type_exists(\"Publication\")\n    \"\"\"\n    return self.doc_service.document_type_exists(doc_type)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.exists","title":"<code>exists(doc_type, id)</code>","text":"<p>Returns a bool value describing whether or not a document exists.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>The document type.</p> required <code>*</code> <code>**id</code> <p>str**</p> <p>The unique id of the document</p> required <p>Example</p> <p>.. code-block:: python</p> <p>context.exists(\"Publication\", \"f33bca95-3caa-46ac-8667-448ba4973190\")</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def exists(self, doc_type, id):\n\"\"\"\n    Returns a bool value describing whether or not a document exists.\n\n    Parameters:\n\n      * **doc_type: str**\n\n          The document type.\n      * **id: str**\n\n          The unique id of the document\n\n\n    Example\n\n    .. code-block:: python\n\n      context.exists(\"Publication\", \"f33bca95-3caa-46ac-8667-448ba4973190\")\n    \"\"\"\n    return self.doc_service.exists(doc_type, id)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.get","title":"<code>get(doc_type, id, source=None, source_exclude=None, source_include=None, wait=False, timeout=60, sentinel_key='_ingest_sentinel', sentinel_value=None)</code>","text":"<p>Returns a document from Elasticsearch.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>The document type of the desired document.</p> required <code>*</code> <code>**id</code> <p>str**</p> <p>The identifier of a single document.</p> required <code>*</code> <code>**source</code> <p>list, defult=None**</p> <p>optional</p> <p>A list containing the fields that will be returned for the document. Must include <code>_id</code> and <code>doc_type</code>.</p> required <code>*</code> <code>**source_exclude</code> <p>list, default=None**</p> <p>optional</p> <p>A list containing the fields that will be excluded from what is returned for the document.</p> required <code>*</code> <code>**source_include</code> <p>list, default=None**</p> <p>optional</p> <p>A list containing the fields that will returned for the document.</p> required <code>*</code> <code>**wait</code> <p>bool, default=False**</p> <p>optional</p> <p>If True, this function will wait to return until the document is searchable in Elasticsearch.</p> required <code>*</code> <code>**timeout</code> <p>int, default=60**</p> <p>optional</p> <p>This specifies the maximum time to wait for the document to be searchable in Elasticsearch if the <code>wait</code> parameter is set to <code>True</code>.</p> required <code>*</code> <code>**sentinel_key</code> <p>str, default='_ingest_sentinel'**</p> <p>optional</p> <p>Advance Usage</p> <p>Elasticsearch field that contains the <code>sentinel_value</code>. By default, there will be an <code>_ingest_sentinal</code> Elasticsearch field.</p> <p>This parameter relates to the <code>barrier</code> concept used as a parameter in the :py:meth:<code>yaada.analytic.context.AnalyticContext.update</code> method. If this <code>get</code> method is being used to retrieve a document in Elasticsearch soon after it has been ingested, this can be used to wait until that value is searchable in Elasticsearch before it is retrieved. This parameter will only be applied if the <code>wait</code> parameter is set to <code>True</code>.</p> required <code>*</code> <code>**sentinel_value</code> <p>str, default=None**</p> <p>optional</p> <p>Advance Usage</p> <p>This parameter is used to ensure that an updated document has fully updated in Elasticsearch before it is retrieved by the <code>get</code> method. Because a document that is being updated may already exist, YAADA needs a way to know that it has been updated before retrieving the document. This is done by introducing a new value to the document, called a sentinal value. If a value is specified for this parameter, YAADA will check that this value matches up with the value in the <code>sentinel_key</code> field for the document before retrieving it, to know that it has been updated.</p> required <p>.. code-block:: python</p> <p>context.get(\"Publication\", \"2cf0d07c-cba9-47e3-9063-5d6265e26089\")</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def get(\n    self,\n    doc_type,\n    id,\n    source=None,\n    source_exclude=None,\n    source_include=None,\n    wait=False,\n    timeout=60,\n    sentinel_key=\"_ingest_sentinel\",\n    sentinel_value=None,\n):\n\"\"\"\n    Returns a document from Elasticsearch.\n\n    Parameters:\n\n      * **doc_type: str**\n\n        The document type of the desired document.\n\n      * **id: str**\n\n        The identifier of a single document.\n\n      * **source: list, defult=None**\n\n        optional\n\n        A list containing the fields that will be returned for the document. Must include ``_id`` and ``doc_type``.\n\n      * **source_exclude:list, default=None**\n\n        optional\n\n        A list containing the fields that will be excluded from what is returned for the document.\n\n      * **source_include:list, default=None**\n\n        optional\n\n        A list containing the fields that will returned for the document.\n\n      * **wait:bool, default=False**\n\n        optional\n\n\n        If True, this function will wait to return until the document is searchable in Elasticsearch.\n\n      * **timeout:int, default=60**\n\n        optional\n\n        This specifies the maximum time to wait for the document to be searchable in Elasticsearch if the ``wait`` parameter is set to ``True``.\n\n      * **sentinel_key:str, default='_ingest_sentinel'**\n\n        optional\n\n        Advance Usage\n\n        Elasticsearch field that contains the ``sentinel_value``. By default, there will be an ``_ingest_sentinal`` Elasticsearch field.\n\n\n        This parameter relates to the ``barrier`` concept used as a parameter in the :py:meth:`yaada.analytic.context.AnalyticContext.update` method.\n        If this ``get`` method is being used to retrieve a document in Elasticsearch soon after it has been ingested, this can be used to wait until\n        that value is searchable in Elasticsearch before it is retrieved. This parameter will only be applied if the ``wait`` parameter\n        is set to ``True``.\n\n      * **sentinel_value:str, default=None**\n\n        optional\n\n        Advance Usage\n\n        This parameter is used to ensure that an updated document has fully updated in Elasticsearch before it is retrieved by the ``get`` method. Because a\n        document that is being updated may already exist, YAADA needs a way to know that it has been updated before retrieving the document.\n        This is done by introducing a new value to the document, called a sentinal value. If a value is specified for this parameter, YAADA will\n        check that this value matches up with the value in the ``sentinel_key`` field for the document before retrieving it, to know that it has been updated.\n\n    .. code-block:: python\n\n      context.get(\"Publication\", \"2cf0d07c-cba9-47e3-9063-5d6265e26089\")\n\n    \"\"\"\n    if wait:\n        self.ingest_barrier(\n            doc_type,\n            id,\n            timeout=timeout,\n            sentinel_key=sentinel_key,\n            sentinel_value=sentinel_value,\n        )\n    return self.doc_service.get(\n        doc_type,\n        id,\n        _source=source,\n        _source_exclude=source_exclude,\n        _source_include=source_include,\n    )\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.index_field_mappings","title":"<code>index_field_mappings(doc_type, *fields)</code>","text":"<p>Returns the Elasticsearch mapping for the given <code>doc_type</code>. The Elasticsearch documentation has a section on <code>mappings. &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html&gt;</code>_</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>The document type, this represents an Elasticsearch index.</p> required <code>*</code> <code>**fields</code> <p>str **</p> <p>optional</p> <p>Limits the mapping to fields given for this parameter. Wildcards can be used.</p> required Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def index_field_mappings(self, doc_type, *fields):\n    # Link out specific Elasticsearch call\n\"\"\"\n    Returns the Elasticsearch mapping for the given ``doc_type``. The Elasticsearch documentation\n    has a section on `mappings. &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html&gt;`_\n\n    Parameters:\n\n      * **doc_type: str**\n\n        The document type, this represents an Elasticsearch index.\n\n      * **fields: str **\n\n        optional\n\n        Limits the mapping to fields given for this parameter. Wildcards can be used.\n    \"\"\"\n    return self.doc_service.index_field_mappings(doc_type, *fields)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.index_mappings","title":"<code>index_mappings(doc_type)</code>","text":"<p>Returns raw index mapping for an Elasticsearch index. Includes Elasticsearch metadata that :py:meth:<code>yaada.analytic.context.AnalyticContext.index_field_mappings</code> does not.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>The document type, this represents an Elasticsearch index.</p> required Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def index_mappings(self, doc_type):\n\"\"\"\n    Returns raw index mapping for an Elasticsearch index. Includes Elasticsearch metadata that :py:meth:`yaada.analytic.context.AnalyticContext.index_field_mappings` does not.\n\n    Parameters:\n\n      * **doc_type: str**\n\n        The document type, this represents an Elasticsearch index.\n\n    \"\"\"\n    return self.doc_service.index_mappings(doc_type)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.index_settings","title":"<code>index_settings(doc_type)</code>","text":"<p>Returns an object with an index's Elasticsearch settings. <code>Documentation on Elasticsearch's settings &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html#index-modules-settings&gt;</code>_</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>The document type, this represents an Elasticsearch index.</p> required Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def index_settings(self, doc_type):\n\"\"\"\n    Returns an object with an index's Elasticsearch settings. `Documentation on Elasticsearch's settings &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html#index-modules-settings&gt;`_\n\n    Parameters:\n\n      * **doc_type: str**\n\n        The document type, this represents an Elasticsearch index.\n\n\n    \"\"\"\n    return self.doc_service.index_settings(doc_type)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.ingest_barrier","title":"<code>ingest_barrier(doc_type, id, timeout=60, sentinel_key='_ingest_sentinel', sentinel_value=None)</code>","text":"<p>Describe sentinal key and value here so that it can be refered to.</p> <p>Parameters:</p> Name Type Description Default <code>doc_type</code> <code>[type]</code> <p>[description]</p> required <code>id</code> <code>[type]</code> <p>[description]</p> required <code>timeout</code> <code>int</code> <p>[description]. Defaults to 60.</p> <code>60</code> <code>sentinel_key</code> <code>str</code> <p>[description]. Defaults to '_ingest_sentinel'.</p> <code>'_ingest_sentinel'</code> <code>sentinel_value</code> <code>[type]</code> <p>[description]. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>IngestBarrierTimeout</code> <p>[description]</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def ingest_barrier(\n    self,\n    doc_type,\n    id,\n    timeout=60,\n    sentinel_key=\"_ingest_sentinel\",\n    sentinel_value=None,\n):\n\"\"\"\n    Describe sentinal key and value here so that it can be refered to.\n\n    Args:\n        doc_type ([type]): [description]\n        id ([type]): [description]\n        timeout (int, optional): [description]. Defaults to 60.\n        sentinel_key (str, optional): [description]. Defaults to '_ingest_sentinel'.\n        sentinel_value ([type], optional): [description]. Defaults to None.\n\n    Raises:\n        IngestBarrierTimeout: [description]\n    \"\"\"\n    delays = [0, 1, 5, 10, 30]\n    delay = 0\n    found = False\n    start_time = time.time()\n    while not found:\n        elapsed = time.time() - start_time\n        if timeout is not None and elapsed &gt;= timeout:\n            raise IngestBarrierTimeout(\n                f\"timeout of {timeout} reached and {doc_type}:{id} not found in total of {elapsed} seconds\"\n            )\n\n        if delays:\n            delay = delays.pop(0)\n\n        if timeout is not None and timeout - elapsed &lt; delay:\n            delay = timeout - elapsed\n        if delay &gt; 0:\n            # print(f\"trying in {delay} seconds\")\n            time.sleep(delay)\n        if sentinel_value is None:\n            found = self.exists(doc_type, id)\n        else:\n            found = self.exists(doc_type, id) and self.sentinel_present(\n                doc_type, id, sentinel_key, sentinel_value\n            )\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.mget","title":"<code>mget(doc_type, ids, source=None, source_exclude=None, source_include=None)</code>","text":"<p>Returns multiple documents from Elasticsearch.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>The document type of the documents to be returned.</p> required <code>*</code> <code>**ids</code> <p>list**</p> <p>List of unique identifiers of documents in Elasticsearch</p> required <code>*</code> <code>**source</code> <p>list, defult=None**</p> <p>optional</p> <p>A list containing the fields that will be returned for the document.</p> required <p>Example:</p> <p>.. code-block:: python</p> <p>context.mget(\"Publication\", [\"2cf0d07c-cba9-47e3-9063-5d6265e26089\", \"b0e062ac-0e84-40db-8ecd-36e1aa0e264b\"])</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def mget(\n    self, doc_type, ids, source=None, source_exclude=None, source_include=None\n):\n\"\"\"\n    Returns multiple documents from Elasticsearch.\n\n    Parameters:\n\n      * **doc_type: str**\n\n        The document type of the documents to be returned.\n\n      * **ids: list**\n\n        List of unique identifiers of documents in Elasticsearch\n\n      * **source: list, defult=None**\n\n        optional\n\n        A list containing the fields that will be returned for the document.\n\n      * **source_exclude, default=None**\n\n        optional\n\n        A list containing the fields that will be excluded from what is returned for the document.\n\n      * **source_include, default=None**\n\n        optional\n\n        A list containing the fields that will returned for the document.\n\n\n    Example:\n\n    .. code-block:: python\n\n      context.mget(\"Publication\", [\"2cf0d07c-cba9-47e3-9063-5d6265e26089\", \"b0e062ac-0e84-40db-8ecd-36e1aa0e264b\"])\n    \"\"\"\n    return self.doc_service.mget(\n        doc_type,\n        ids,\n        _source=source,\n        _source_exclude=source_exclude,\n        _source_include=source_include,\n    )\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.query","title":"<code>query(doc_type, query={'query': {'match_all': {}}}, size=None, scroll_size=1000, scroll='2m', raw=False, source=None)</code>","text":"<p>Returns results of a query to Elasticsearch as a generator object. It uses an abstraction of the <code>scroll API &lt;https://www.elastic.co/guide/en/elasticsearch/reference/7.9/scroll-api.html&gt;</code>_.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>This represents an Elasticsearch index. The query will search the index.</p> <p>Every document in Elasticsearch has a required <code>doc_type</code> field as described in the :ref:<code>Documents</code> section.</p> required <code>*</code> <code>**query</code> <p>dict, default={\"query\": {\"match_all\": {}}}**</p> <p>optional</p> <p>An Elasticsearch query. More info can be found in <code>Elasticsearch documentation. &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html&gt;</code>_</p> required <code>*</code> <code>**size</code> <p>int, default=None**</p> <p>optional</p> <p>Specifies number of documents to be returned. If the value is greater than the resulting number of documents, the resulting documents will all be returned.</p> required <code>*</code> <code>**scroll_size</code> <p>int, defaults=1000**</p> <p>optional</p> required <code>*</code> <code>**scroll</code> <p>str, default='2m'**</p> <p>optional</p> required <code>*</code> <code>**raw</code> <p>bool, default=False**</p> <p>optional</p> required <code>*</code> <code>**source</code> <p>list, default=None**</p> <p>optional</p> <p>A list of fields that will be shown for each document returned by the query. Used to limit the fields that each document will return. Must include <code>_id</code> and <code>doc_type</code>.</p> required Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def query(\n    self,\n    doc_type,\n    query={\"query\": {\"match_all\": {}}},\n    size=None,\n    scroll_size=1000,\n    scroll=\"2m\",\n    raw=False,\n    source=None,\n):\n\"\"\"\n    Returns results of a query to Elasticsearch as a generator object. It uses an abstraction of the `scroll API &lt;https://www.elastic.co/guide/en/elasticsearch/reference/7.9/scroll-api.html&gt;`_.\n\n    Parameters:\n\n      * **doc_type: str**\n\n        This represents an Elasticsearch index. The query will search the index.\n\n        Every document in Elasticsearch has a required ``doc_type`` field as described in the :ref:`Documents` section.\n\n      * **query: dict, default={\"query\": {\"match_all\": {}}}**\n\n        optional\n\n        An Elasticsearch query. More info can be found in `Elasticsearch documentation.\n        &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html&gt;`_\n      * **size:int, default=None**\n\n        optional\n\n        Specifies number of documents to be returned. If the value is greater than the\n        resulting number of documents, the resulting documents will all be returned.\n      * **scroll_size:int, defaults=1000**\n\n        optional\n\n      * **scroll:str, default='2m'**\n\n        optional\n\n      * **raw:bool, default=False**\n\n        optional\n\n      * **source:list, default=None**\n\n        optional\n\n        A list of fields that will be shown for each document returned by the query. Used to limit the fields that each document will return.\n        Must include ``_id`` and ``doc_type``.\n\n    \"\"\"\n\n    if query is None:\n        query = {\"query\": {\"match_all\": {}}}\n    if source is not None:\n        query[\"_source\"] = source\n    for d in self.doc_service.query(\n        doc_type, query, size=size, scroll_size=scroll_size, scroll=scroll, raw=raw\n    ):\n        if not raw:\n            self._count_input(d[\"doc_type\"])\n        yield d\n    self.report_status()\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.rawquery","title":"<code>rawquery(doc_type, query)</code>","text":"<p>Returns results to a query.</p> <p>This returns the bare elasticsearch results. Use this to access to the scoring information or aggregations.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>The document type, this represents an Elasticsearch index.</p> required <code>*</code> <code>**query</code> <p>dict**</p> <p>Elasticsearch query, more documentation can be found <code>here &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html&gt;</code>_.</p> required Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def rawquery(self, doc_type, query):\n\"\"\"\n    Returns results to a query.\n\n    This returns the bare elasticsearch results. Use this to access to the scoring information or aggregations.\n\n    Parameters:\n\n      * **doc_type: str**\n\n        The document type, this represents an Elasticsearch index.\n\n      * **query: dict**\n\n        Elasticsearch query, more documentation can be found `here &lt;https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html&gt;`_.\n\n    \"\"\"\n    return self.doc_service.rawquery(doc_type, query)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.report_status","title":"<code>report_status(write_es=False, message=None)</code>","text":"<p>Called automatically when analytics are started and finishing. Available to be called by Anaytic developers to show intermediate status of a an analytic. To do this, call it in the <code>run</code> function of an Analytic.</p> <p>Parameters:</p> <ul> <li> <p>write_es:bool, default=False</p> </li> <li> <p>message:str, default=None</p> <p>A string value containing the message to display.</p> </li> </ul> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def report_status(self, write_es=False, message=None):\n\"\"\"\n    Called automatically when analytics are started and finishing. Available to be called by Anaytic developers\n    to show intermediate status of a an analytic. To do this, call it in the ``run`` function of an Analytic.\n\n      Parameters:\n\n      * **write_es:bool, default=False**\n\n\n\n      * **message:str, default=None**\n\n        A string value containing the message to display.\n\n    \"\"\"\n    self.status[\"@timestamp\"] = datetime.utcnow()\n    self.status[\"analytic_compute_duration_seconds\"] = (\n        datetime.utcnow() - self._start_time\n    ).total_seconds()\n    if message is not None:\n        self.status[\"message\"] = message\n        if self._printer is not None:\n            self._printer(message)\n    self.msg_service.publish_analytic_status(\n        self.analytic_name, self.analytic_session_id, self.status\n    )\n    if write_es:\n        self.doc_service.write_analytic_status(\n            self.analytic_name, self.analytic_session_id, self.status\n        )\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.sentinel_present","title":"<code>sentinel_present(doc_type, id, sentinel_key, sentinel_value)</code>","text":"<p>check if a document is searchable with a given sentinel value.</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def sentinel_present(self, doc_type, id, sentinel_key, sentinel_value):\n\"\"\"check if a document is searchable with a given sentinel value.\"\"\"\n    docs = list(\n        self.query(\n            doc_type,\n            {\n                \"query\": {\n                    \"bool\": {\n                        \"must\": [\n                            {\"term\": {\"_id\": id}},\n                            {\"term\": {f\"{sentinel_key}.keyword\": sentinel_value}},\n                        ]\n                    }\n                }\n            },\n        )\n    )\n    if len(docs) &gt; 0:\n        doc = docs[0]\n        if sentinel_key in doc:\n            return doc[sentinel_key] == sentinel_value\n\n    return False\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.set_analytic_name","title":"<code>set_analytic_name(analytic_name)</code>","text":"<p>Assigns an <code>analytic_name</code> value of a context object.</p> <ul> <li>analytic_name: str</li> </ul> <p>A description of the analytic's purpose</p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code>context.set_analytic_name(\"jupyter\")\n</code></pre> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def set_analytic_name(self, analytic_name):\n\"\"\"\n    Assigns an ``analytic_name`` value of a context object.\n\n    Parameters:\n\n    * **analytic_name: str**\n\n      A description of the analytic's purpose\n\n    Example:\n\n    .. code-block:: python\n\n        context.set_analytic_name(\"jupyter\")\n    \"\"\"\n    self.analytic_name = analytic_name\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.set_analytic_session_id","title":"<code>set_analytic_session_id(analytic_session_id)</code>","text":"<p>Assigns an <code>analytic_session_id</code> value of a context object.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**analytic_session_id</code> <p>str**</p> <p>An identifier for this analytic object</p> required <p>Example:</p> <p>.. code-block:: python</p> <pre><code>context.set_analytic_session_id(\"Query Data\")\n</code></pre> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def set_analytic_session_id(self, analytic_session_id):\n\"\"\"\n    Assigns an ``analytic_session_id`` value of a context object.\n\n    Parameters:\n\n      * **analytic_session_id: str**\n\n        An identifier for this analytic object\n\n    Example:\n\n    .. code-block:: python\n\n        context.set_analytic_session_id(\"Query Data\")\n    \"\"\"\n    self.analytic_session_id = analytic_session_id\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.sync_exec_analytic","title":"<code>sync_exec_analytic(analytic_name, analytic_session_id=None, parameters={}, include_results=False, printer=print)</code>","text":"<p>Synchronously run an analytic. A tutorial on writing and running yaada analytics can be found in the :ref:<code>Writing an Analytic&lt;Writing an Analytic&gt;</code> section.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**analytic_name</code> <p>str**</p> <p>The full analytic class name.</p> required <code>*</code> <code>**analytic_session_id</code> <p>str, default=None**</p> <p>optional</p> <p>This value will be stored in Elasticsearch to mark the documents changed by this analytic session. If a value is not defined, the context object's <code>analytic_session_id</code> value will be automatically assigned.</p> required <code>*</code> <code>**parameters</code> <p>dict, default={}**</p> <p>optional</p> <p>This value corresponds to the parameters of the yaada analytic being run.</p> <p>The parameters are assigned by setting each key in this dict to a string of the name of a parameter. The parameter will be assinged to the value given in the key-value pair.</p> required <code>*</code> <code>**include_results</code> <p>bool, default=False**</p> <p>If running an analytic synchronously, setting this to  <code>True</code> will return the produced documents from the analytic.</p> <p>Warning - Will Produce a Large Output</p> <p>optional</p> required <code>*</code> <code>**printer</code> <p>function, default=print**</p> <p>optional</p> <p>Advanced Usage, Experimental API subject to change, not recommended for users</p> required <p>Example:</p> <p>.. code-block:: python</p> <p>context.sync_exec_analytic(       analytic_name=\"yaada.analytic.builtin.newspaper.NewspaperScrapeSources\",       parameters={\"content\":True,\"scrape_top_image\":False,\"memoize\":True})</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def sync_exec_analytic(\n    self,\n    analytic_name,\n    analytic_session_id=None,\n    parameters={},\n    include_results=False,\n    printer=print,\n):\n\"\"\"\n    Synchronously run an analytic. A tutorial on writing and running yaada analytics can be found in the :ref:`Writing an Analytic&lt;Writing an Analytic&gt;` section.\n\n    Parameters:\n\n      * **analytic_name: str**\n\n        The full analytic class name.\n\n      * **analytic_session_id: str, default=None**\n\n        optional\n\n        This value will be stored in Elasticsearch to mark the documents changed by this analytic session.\n        If a value is not defined, the context object's ``analytic_session_id`` value will be automatically assigned.\n\n      * **parameters: dict, default={}**\n\n        optional\n\n        This value corresponds to the parameters of the yaada analytic being run.\n\n        The parameters are assigned by setting each key in this dict to a string\n        of the name of a parameter. The parameter will be assinged to the value given in the key-value pair.\n\n      * **include_results: bool, default=False**\n\n        If running an analytic synchronously, setting this to  ``True`` will return the produced documents from the analytic.\n\n        Warning - Will Produce a Large Output\n\n        optional\n\n      * **printer: function, default=print**\n\n        optional\n\n        Advanced Usage, Experimental API subject to change, not recommended for users\n\n    Example:\n\n    .. code-block:: python\n\n      context.sync_exec_analytic(\n          analytic_name=\"yaada.analytic.builtin.newspaper.NewspaperScrapeSources\",\n          parameters={\"content\":True,\"scrape_top_image\":False,\"memoize\":True})\n\n    \"\"\"\n    if analytic_session_id is None:\n        analytic_session_id = str(_uuid.uuid4())\n    c = self.create_derived_context(analytic_name, analytic_session_id, parameters)\n    return sync_exec_analytic(\n        c,\n        analytic_name,\n        analytic_session_id,\n        parameters,\n        include_results=include_results,\n        printer=printer,\n    )\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.term_counts","title":"<code>term_counts(doc_type, term, query={'query': {'match_all': {}}})</code>","text":"<p>Returns an object describing the frequency of terms for a given field of an index by running an Elasticsearch terms aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <code>**doc_type</code> <p>str**</p> <p>The document type, this represents an Elasticsearch index.</p> required <code>*</code> <code>**term</code> <p>str**</p> <p>The field that will be inspected.</p> required <code>*</code> <code>**query</code> <p>dict, default={\"query\":{\"match_all\":{}}}**</p> <p>optional</p> <p>Provides a filter for the documents that will be aggregated.</p> required Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def term_counts(self, doc_type, term, query={\"query\": {\"match_all\": {}}}):\n\"\"\"\n\n    Returns an object describing the frequency of terms for a given field of an index by running an Elasticsearch terms aggregation.\n\n    Parameters:\n\n      * **doc_type: str**\n\n        The document type, this represents an Elasticsearch index.\n\n      * **term: str**\n\n        The field that will be inspected.\n\n      * **query: dict, default={\"query\":{\"match_all\":{}}}**\n\n        optional\n\n        Provides a filter for the documents that will be aggregated.\n\n    \"\"\"\n    return self.doc_service.term_counts(doc_type, term, query)\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.AnalyticContext.update","title":"<code>update(doc, process=True, sync=True, archive=False, barrier=False, barrier_timeout=600, raise_ingest_error=True, validate=True)</code>","text":"<p>Stores the document given by the <code>doc</code> parameter in Elasticsearch.</p> <p>Parameters:</p> <ul> <li> <p>doc: dict, list[dict]</p> <p>The <code>doc</code> parameter, represents an Elasticsearch document to be ingested. The schema and details of a document can be found in :ref:<code>Documents</code>.</p> <p>This parameter can be given as a dictionary representation of an Elasticsearch document, or a list of <code>doc</code>'s which will result in a batch-ingest.</p> </li> <li> <p>process: bool, default = True</p> <p>optional</p> <p>If <code>True</code>, the update process with run through the ingest pipeline</p> </li> <li> <p>sync: bool, default = True</p> <p>optional</p> <p>If value is set to <code>True</code>, the process will be run synchronously, if set to <code>False</code>, it will run asynchronously.</p> </li> <li> <p>archive: bool, default = False</p> <p>optional</p> <p>If the document or documents being ingested are from an archive, this parameter's value should be set to <code>True</code>. Otherwise, it will default to <code>False</code>.</p> <p>When <code>False</code>, a document being ingested through the <code>update</code> function will be automtically set a timestamp value to the field <code>@timestamp</code> to the same time that it was ingested. When <code>True</code>, because the document is from an archive, the <code>@timestamp</code> field will already exists and will not be reassigned. Therefore, it will keep it's value representing the time when it was originally ingested.</p> </li> <li> <p>barrier: bool, default=False</p> <p>optional</p> <p>Setting <code>barrier</code> to <code>True</code> ensures that the document is indexed Elasticsearch and searchable before the the update function finishes running.</p> <p>Enabling <code>barrier</code> will dractically reduce throughput performace especially if <code>update</code> is being called repeatedly by updating one document at a time. The process will not be slowed drastically if the <code>update</code> is run as a batch-ingest.</p> </li> <li> <p>barrier_timeout: int, default=600</p> <p>optional</p> <p>This value is the maximum amount of time in seconds that the function will wait for the document to appear in Elasticsearch before finishing. If this maximum value is reached and the document is not in elastic search, it will return an <code>IngestBarrierTimeout</code> error. This only applies if the <code>barrier</code> parameter is set to <code>True</code>.</p> </li> <li> <p>raise_ingest_error: bool, default=True</p> <p>optional</p> </li> </ul> <p>Short example:</p> <p>.. code-block:: python</p> <p>doc = {       _id = \"123\",       doc_type = \"EmployeeSurvey\",       favorite_color = \"red\",       favoite_number = 4   }   context.update(doc)</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def update(\n    self,\n    doc,\n    process=True,\n    sync=True,\n    archive=False,\n    barrier=False,\n    barrier_timeout=600,\n    raise_ingest_error=True,\n    validate=True,\n):\n\"\"\"\n\n    Stores the document given by the ``doc`` parameter in Elasticsearch.\n\n      Parameters:\n\n      * **doc: dict, list[dict]**\n\n        The ``doc`` parameter, represents an Elasticsearch document to be ingested. The schema and details of a document can be found in :ref:`Documents`.\n\n        This parameter can be given as a dictionary representation of an Elasticsearch document, or a list of ``doc``'s which will result in a batch-ingest.\n\n      * **process: bool, default = True**\n\n        optional\n\n        If ``True``, the update process with run through the ingest pipeline\n\n      * **sync: bool, default = True**\n\n        optional\n\n        If value is set to ``True``, the process will be run synchronously, if set to ``False``, it will run asynchronously.\n\n      * **archive: bool, default = False**\n\n        optional\n\n        If the document or documents being ingested are from an archive, this parameter's value should be set\n        to ``True``. Otherwise, it will default to ``False``.\n\n        When ``False``, a document being ingested through the ``update`` function will be automtically set a timestamp value\n        to the field ``@timestamp`` to the same time that it was ingested. When ``True``, because the document is from an archive,\n        the ``@timestamp`` field will already exists and will not be reassigned. Therefore, it will keep it's value representing the time\n        when it was originally ingested.\n\n      * **barrier: bool, default=False**\n\n        optional\n\n        Setting ``barrier`` to ``True`` ensures that the document is indexed Elasticsearch\n        and searchable before the the update function finishes running.\n\n        Enabling ``barrier`` will dractically reduce throughput performace especially if ``update``\n        is being called repeatedly by updating one document at a time. The process will not be slowed\n        drastically if the ``update`` is run as a batch-ingest.\n\n      * **barrier_timeout: int, default=600**\n\n        optional\n\n        This value is the maximum amount of time in seconds that the function will wait\n        for the document to appear in Elasticsearch before finishing. If this maximum value\n        is reached and the document is not in elastic search, it will return an\n        ``IngestBarrierTimeout`` error. This only applies if the ``barrier`` parameter is set\n        to ``True``.\n\n      * **raise_ingest_error: bool, default=True**\n\n        optional\n\n\n    Short example:\n\n    .. code-block:: python\n\n      doc = {\n          _id = \"123\",\n          doc_type = \"EmployeeSurvey\",\n          favorite_color = \"red\",\n          favoite_number = 4\n      }\n      context.update(doc)\n\n\n    \"\"\"\n    self.result(\n        doc,\n        process=process,\n        sync=sync,\n        upsert=True,\n        archive=archive,\n        barrier=barrier,\n        barrier_timeout=barrier_timeout,\n        raise_ingest_error=raise_ingest_error,\n        validate=validate,\n    )\n</code></pre>"},{"location":"api/#yaada.core.analytic.context.make_analytic_context","title":"<code>make_analytic_context(analytic_name='anonymous', analytic_session_id=None, parameters={}, msg_service=None, doc_service=None, ob_service=None, model_manager=None, init_pipelines=True, init_analytics=True, init_models=True, config=None, schema_manager=None, connect_to_services=True, overrides={})</code>","text":"<p>Return an instance of the context object. A context object is needed to interact with the Elasticsearch database using python. A detailed description of the context object can be found in the :ref:<code>Context Object&lt;Context Object&gt;</code> section.</p> <ul> <li>analytic_name: str, default='anonymous'</li> </ul> <p>A string describing the reason for this context object's instantiation.   This value will mark documents created and edited by this instance of the context object</p> <p>When running an analytic, this value will automatically get set to the analytic's class name.</p> <p>This value will be stored in a document's <code>analytic_name</code> field.</p> <ul> <li>analytic_session_id: str, default=None</li> </ul> <p>optional</p> <p>This value will be saved in Elasticsearch. It will mark which documents are edited by this specific instance of the   context object.   Documents added or edited by a context, will inherit that context's <code>analytic_session_id</code> in a <code>analytic_session_id</code> field.</p> <p>If no value is given, a universally unique identifier will be automatically assigned.</p> <ul> <li>parameters:dict, default={}</li> </ul> <p>private * msg_service=None</p> <p>private * doc_service=None</p> <p>private * ob_service=None</p> <p>private * model_manager=None</p> <ul> <li>init_pipelines=True</li> </ul> <p>optional</p> <p>When <code>True</code>, the creation of the context object will initialize the pipelines. The pipeline needs to be initialized for   asynchronous processes.</p> <p>The only downside to initializing the pipeline when it is not necessary is that it will take a little longer to create the context object. * config=None</p> <p>private * schema_manager=None</p> <p>private * overrides={}</p> <p>optional</p> <p>Allows an override to environment variables and settings in a service.</p> <p>Here is an example of what is needed to get started using the <code>context object</code></p> <p>.. code-block:: python</p> <p>from yaada.analytic.context import make_analytic_context   context = make_analytic_context()</p> Source code in <code>/Users/ganberg/dev/git/yaada/src/core/yaada/core/analytic/context.py</code> <pre><code>def make_analytic_context(\n    analytic_name=\"anonymous\",\n    analytic_session_id=None,\n    parameters={},\n    msg_service=None,\n    doc_service=None,\n    ob_service=None,\n    model_manager=None,\n    init_pipelines=True,\n    init_analytics=True,\n    init_models=True,\n    config=None,\n    schema_manager=None,\n    connect_to_services=True,\n    overrides={},\n):\n\"\"\"\n    Return an instance of the context object. A context object is needed to\n    interact with the Elasticsearch database using python. A detailed description of the\n    context object can be found in the :ref:`Context Object&lt;Context Object&gt;` section.\n\n    Parameters:\n\n    * **analytic_name: str, default='anonymous'**\n\n      A string describing the reason for this context object's instantiation.\n      This value will mark documents created and edited by this instance of the context object\n\n      When running an analytic, this value will automatically get set to the analytic's class name.\n\n      This value will be stored in a document's ``analytic_name`` field.\n\n    * **analytic_session_id: str, default=None**\n\n      optional\n\n      This value will be saved in Elasticsearch. It will mark which documents are edited by this specific instance of the\n      context object.\n      Documents added or edited by a context, will inherit that context's ``analytic_session_id`` in a ``analytic_session_id`` field.\n\n      If no value is given, a universally unique identifier will be automatically assigned.\n\n    * **parameters:dict, default={}**\n\n      private\n    * **msg_service=None**\n\n      private\n    * **doc_service=None**\n\n      private\n    * **ob_service=None**\n\n      private\n    * **model_manager=None**\n\n    * **init_pipelines=True**\n\n      optional\n\n      When ``True``, the creation of the context object will initialize the pipelines. The pipeline needs to be initialized for\n      asynchronous processes.\n\n      The only downside to initializing the pipeline when it is not necessary is that it will take a little longer to create the context object.\n    * **config=None**\n\n      private\n    * **schema_manager=None**\n\n      private\n    * **overrides={}**\n\n      optional\n\n      Allows an override to environment variables and settings in a service.\n\n    Here is an example of what is needed to get started using the ``context object``\n\n    .. code-block:: python\n\n      from yaada.analytic.context import make_analytic_context\n      context = make_analytic_context()\n\n\n    \"\"\"\n    start_t = time.time()\n    if analytic_session_id is None:\n        analytic_session_id = str(_uuid.uuid4())\n\n    context = AnalyticContext(\n        analytic_name,\n        analytic_session_id,\n        parameters,\n        msg_service=msg_service,\n        doc_service=doc_service,\n        ob_service=ob_service,\n        model_manager=model_manager,\n        config=config,\n        schema_manager=schema_manager,\n        connect_to_services=connect_to_services,\n        overrides=overrides,\n    )\n    analytic.find_and_register(\n        context.config, init_analytics, init_pipelines, init_models\n    )\n\n    if init_pipelines:\n        context.init_pipeline()\n\n    end_t = time.time()\n    logger.info(f\"##context creation took {end_t-start_t} seconds\")\n    return context\n</code></pre>"},{"location":"concepts/","title":"Concepts","text":"<p>YAADA is a cloud-based data architecture and analytics platform designed to support the full analytics development spectrum, from prototyping to deployment-at-scale. YAADA\u2019s primary focus is ingesting, storing, analyzing, and producing semi-structured document-oriented data.</p>"},{"location":"concepts/#yaada-architecture","title":"YAADA Architecture","text":"YAADA Architecture <p>YAADA is a framework for handling and analyzing data. YAADA utilizes two data stores \u2014 Elasticsearch and S3 object storage \u2014 to provide services that allow for analysis and processing of data through Python-based analytics.</p> <p>YAADA uses a containerized approach through Docker. A standard YAADA project will have the following Docker containers:</p> Container Function ingest Handles asynchronous per-document processing at ingest time sink Handles sending data asynchronously to Elasticsearch worker Handles asynchronous execution of analytics openapi OpenAPI-based REST API elasticsearch 3rd Party primary data store kibana 3rd Party data exploration/dashboard app provided by Elastic mosquitto 3rd Party MQTT messaging for communication between YAADA components zenko 3rd Party S3 style object storage for models and file data tika* Optional 3rd Party service to parse text from a range of file types jupyter* Optional 3rd Party jupyter lab frontend for testing and running python code"},{"location":"concepts/#elasticsearch-and-documents-and-data-ingest","title":"Elasticsearch and Documents and Data Ingest","text":"<p>Elasticsearch is the main database used to store and retrieve data in YAADA. Data in Elasticsearch is stored in documents, and can be queried using the Elasticsearch Query DSL, a query language based on JSON.</p> <p>Elasticsearch documents are the primary form of data that YAADA analytics operate over. Documents are semi-structured, JSON-like objects comprised of key/value pairs. They are represented as Python dictionaries in memory and transmitted across services as json-serialized objects.</p> <p>All YAADA documents will at a mimimum have two required attributes:</p> Field Name Type Description <code>doc_type</code> String The name of the type of document being represented. Used to determine the Elasticsearch index where the document will be stored. <code>id</code> String The unique (within doc_type) id for the document. Used as Elasticsearch id. Autogenerated as a UUID if ommitted. <code>@timestamp</code> String ISO 8601 formatted time that the document was ingested into Elasticsearch. Will be autogenerated. <p>The format of a minimal YAADA document in python would look like this:</p> <pre><code>{\n\"doc_type\":\"Foo\",\n\"id\":\"1\",\n\"foo\":\"bar\"\n}\n</code></pre> <p>With a default configuration, this document would be stored in the <code>yaada-{TENANT}-document-foo</code> index with an Elasticsearch unique id of \u201c1\u201d.</p>"},{"location":"concepts/#schemas","title":"Schemas","text":"<p>YAADA supports defining document schemas as json or yaml files following the OpenAPI 3.0 json-schema variant.</p>"},{"location":"concepts/#analytics","title":"Analytics","text":"<p>A YAADA Analytic in it\u2019s most general form is a parameterized function that takes in <code>0-n</code> documents and produces <code>0-m</code> documents.</p> <p>Some common Analytics types are:</p> <ul> <li>acquire data \u2013 analytics that produce documents representing some external content such as news articles or data housed in an external API.</li> <li>transform data \u2013 analytics that take in a set of documents and emit updated versions of those documents (think Map from Map/Reduce).</li> <li>aggregate data \u2013 analytics that take in a set of documents, perform an analysis and produce a new document representing the aggregate analysis (think Reduce from Map/Reduce).</li> </ul> <p>Generally, most analytics will follow the data flow pattern shown here:</p> <p> </p> The typical flow of document data in an analytic"},{"location":"concepts/#analytic-context","title":"Analytic Context","text":"<p>YAADA provides a built-in <code>yaada.core.analytic.context.AnalyticContext</code> class that serves as the main API for interacting with or analyzing data. Most YAADA services will construct a single <code>context</code> object which will get used by any analytics or pipelines invoked to interact with the backend databases. A user can instantiated as a <code>context</code> object using the <code>yaada.core.analytic.context.make_analytic_context</code> method. Once instantiated, the context object provides a suite of methods for interacting with Elasticsearch, such as querying, fetching, and ingesting documents, as well as accessing YAADA capabilities like running analytics.</p>"},{"location":"concepts/#analytic-models","title":"Analytic Models","text":"<p>Some analytics may be able to produce results in a stateless manner through simple inspection of the incoming documents, but often an analytic will have to use some outside resources to produce results, such as a trained language model. YAADA analytics may define an optional analytic model building step, where the analytic builds a named model from a set of documents and writes it out to a file or set of files to be stored in object storage, as illiustrated here.</p> <p> </p> Training and storing an analytic model based on documents <p>Models created in this way can then be loaded from object storage and used by analytics to produce results, whether that be the analytic that created it or another analytic. This is generally done by querying documents, using the model to analyze those documents, and then ingesting the results back into Elasticsearch, as shown in the following diagram. When an analytic uses a model, all produced results will be annotated with the name of the analytic model they were produced by.</p> <p> </p> Using a pretrained analytic model to analyze documents and produce new results"},{"location":"concepts/#pipeline-processors","title":"Pipeline Processors","text":"<p>Pipelines allow you to run a series of transform functions on documents of a specific <code>doc_type</code> each time such a document is ingested. Each pipeline correspond to one <code>doc_type</code>. If a pieline is configured for a <code>doc_type</code>, upon ingest documents of that type are run through the series of processors configured for that pipeline. A processor, similarly to a single-document analytic, takes in a document and modifies or adds new data to it before sending it off to the next step in the pipeline. If and when all the steps of a pipeline are complete, the document is then sent to be ingested into Elasticsearch.</p> <p> </p> Ingested documents are run through a processing pipeline and then stored into Elasticsearch"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>docker-compose (isn't always installed with Docker under Linux, so may need to install seperately)</li> <li>Python 3.8 installed locally and runnable with name <code>python</code></li> <li>On Windows, make sure Microsoft Visual C++ Build Tools are installed (requires Admin rights) and on the System PATH. (current download location: https://visualstudio.microsoft.com/downloads/ under <code>Tools for Visual Studio 2017</code> ... <code>Build Tools for Visual Studio 2017</code>)</li> <li>Python 3.8</li> <li>PIP</li> <li><code>pipenv</code></li> <li>Optional but recommended <code>cookiecutter</code> -- if you would like to create your project from a template</li> </ul> <p>If you run into pip version issues when installing <code>pipenv</code>, consider installing <code>pipenv</code> through pipx.</p>"},{"location":"getting-started/#docker-setup","title":"Docker Setup","text":"<ul> <li>On Mac, use Docker for Mac</li> <li>On Linux, use Docker for Linux</li> <li>On Windows 10, use Docker for Windows (WSL2 install preferred)</li> </ul>"},{"location":"getting-started/#yaada-memory-requirements","title":"YAADA memory requirements","text":"<p>A full YAADA-based system running infrastructure and YAADA services in development mode should ideally be allocated at least 8GB of memory. If using Docker for Mac, the default virtual machine has 2GB allocated, so this will need to be adjusted.</p>"},{"location":"getting-started/#create-new-project","title":"Create new project:","text":"<p>First, you will need to install <code>cookiecutter</code>:</p> <pre><code>$ pip install cookiecutter\n</code></pre> <p>Next, navigate to the parent directory of where you would like your new project to go, and invoke cookiecutter with the template you want to use:</p> <pre><code>$ cookiecutter https://github.com/Aptima/yaada.git --checkout=v6.2.2 --directory=template/simple-compose\n</code></pre> <p>Follow the prompts, and if successful, you will have a new yaada project directory. All remaining commands will happen from within the project directory</p>"},{"location":"getting-started/#python-environment-setup","title":"Python environment setup","text":""},{"location":"getting-started/#create-virtual-environment","title":"Create virtual environment","text":"<p>Open a virtual environment shell:</p> <pre><code>$ pipenv shell\n</code></pre>"},{"location":"getting-started/#install-the-current-project-into-virtual-environment","title":"Install the current project into virtual environment","text":"<p>This will install the current project's package, as well as the main YAADA packages. This will take a while the first time you to it because of the number of dependencies and the locking process.</p> <pre><code>$ pipenv install\n</code></pre>"},{"location":"getting-started/#building-and-running-through-docker","title":"Building and running through Docker","text":""},{"location":"getting-started/#build-project-images","title":"Build project images","text":"<pre><code>$ yda build\n</code></pre>"},{"location":"getting-started/#launching","title":"Launching","text":"<p>Bring up services and infrastructure:</p> <pre><code>$ yda up\n</code></pre> <p>You can now access the services provided in YAADA. Here is a table of the services and how to access them locally</p> Server Access Points OpenAPI REST UI Jupyter Lab Kibana Zenko CloudServer <p>Bring down services and infrastructure:</p> <pre><code>$ yda down\n</code></pre>"},{"location":"getting-started/#useful-cli-commands","title":"Useful CLI Commands","text":"<p>This section will cover some useful builtin commands and tools for local development.</p> <p>To see what docker containers are running, run:</p> <pre><code>$ yda ps \n</code></pre> <p>When having difficulty with a service, to see the logs of that service, run::</p> <pre><code>$ yda logs &lt;service_name&gt;\n</code></pre> <p>The following command is commonly used to check what documents are currently in Elasticsearch:</p> <pre><code>$ yda data counts\n</code></pre> <p>Launch an IPython shell with a YAADA <code>context</code> already constructed and available and live reload setup:</p> <pre><code>$ yda run ipython\n</code></pre> <p>Launch Jupyter Lab locally (stopping the Docker-based instance that gets launched automatically):</p> <pre><code>$ yda run jupyter\n</code></pre>"},{"location":"openapi/","title":"OpenAPI Documentation for YAADA","text":""},{"location":"tutorial/","title":"Tutorial","text":""},{"location":"tutorial/#creating-a-project","title":"Creating a project","text":"<p>In order to use YAADA, you first need to create a new project directory that will house your project metadata, Docker configuration for running the system, and any project specific source code or schemas. All of the tutorials in this section assume that you have read Getting Started and followed the instructions to install prerequisites, create a project setting <code>yaada-tutorial</code> as the project_name (and accepting all defaults after that), run the system, and are running commands from the root of the project directory.</p>"},{"location":"tutorial/#working-with-jupyter-or-ipython","title":"Working with Jupyter or IPython","text":""},{"location":"tutorial/#jupyter","title":"Jupyter","text":"<p>Jupyter is helpful a developer tool. Many YAADA developers use it when writing the code of an analytic or processor before tying it into the rest of YAADA, or testing a query. To start up the dev Jupyter server, run the following command:</p> <pre><code>$ yda run jupyter\n</code></pre> <p>This will direct you to a new browser tab with the Jupyter interface and show logs in the terminal. If you\u2019re unfamiliar with Jupyter, refer to the Jupyter Lab documentation.</p> <p>Note that when using Jupyter, changes to code outside the notebook will not be picked up until the Jupyter Kernel is restarted.</p> <p>In order to start interacting with YAADA data from a Jupyter noetbook, you will need to add the following as the first cell and execute it:</p> <pre><code>from yaada,core.analytic.context import make_analytic_context\ncontext = make_analytic_context()\n</code></pre>"},{"location":"tutorial/#ipython","title":"IPython","text":"<p>If you would prefer to experiment with YAADA from the command line, the IPython shell provides a REPL that can be used to experiment with the YAADA APIs.</p> <p>Launch IPython with:</p> <pre><code>$ yda run ipython\n</code></pre> <p>Unlike with Jupyter, you don't need to construct the <code>context</code> manually. Once you are in the REPL, you will have a <code>context</code> already constructed for you and any code changes in your project will be automatically picked up through IPython magic autoreload.</p>"},{"location":"tutorial/#ingesting-data","title":"Ingesting Data","text":"<p>This section will cover ingesting data into YAADA's Elasticsearch. The tutorial will leverage a publically available csv containing avengers character data. Content in this section assumes you have created an IPython shell with <code>yda run ipython</code> or using Jupyter Lab with <code>yda run jupyter</code> and have already constructed your YAADA <code>context</code>.</p>"},{"location":"tutorial/#download-csv-data","title":"Download csv data","text":"<p>Download the <code>avengers.csv</code>:</p> <pre><code>import requests\nresponse = requests.get(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/avengers/avengers.csv\")\nopen(\"avengers.csv\", \"wb\").write(response.content)\n</code></pre>"},{"location":"tutorial/#load-the-csv","title":"Load the csv","text":"<p>Read the CSV as dictionaries and write into YAADA. <pre><code>import csv\nimport re\nfrom tqdm import tqdm\nfrom yaada.core.utility import hash_for_text\n\nwith open('avengers.csv', 'r', encoding='latin-1') as csvfile:\n  reader = csv.DictReader(csvfile)\n  # Iterate through the csv, printing progress with tqdm\n  for row in tqdm(reader): \n    # Create a dictionary containing all the values from the row, adding a `doc_type`\n    # and normalizing the csv columns names to be better dictionary keys. Note that any \n    # fields that are an empty string are omitted to prevent Elasticsearch from inferring \n    # the wrong schema type for the field.\n    doc = dict(doc_type=\"Avenger\",**{re.sub(r\"\\W+\",'_',k.lower()):v for k,v in row.items() if v != ''})\n    # for fields that contain numeric values, coerce to number if non empty string, delete if\n    if doc.get('appearances',''):\n        doc['appearances'] = int(doc['appearances'])\n    if doc.get('year',''):\n        doc['year'] = int(doc['year'])\n    if doc.get('years_since_joining',''):\n        doc['years_since_joining'] = int(doc['years_since_joining'])\n\n    # Create a unique id for this row by hashing the URL it came from.\n    doc['id'] = hash_for_text([row['URL']]) \n    # Save into elasticsearch\n    context.update(doc) \n</code></pre></p> <p>Now verify that you have documents in Elasticsearch using <code>context.document_counts()</code>:</p> <pre><code>context.document_counts()\n</code></pre> <p>will result in: <pre><code>{'Avenger': 173}\n</code></pre></p> <p>Now to look at what the data looks like once ingested, fetch all the <code>Avenger</code> documents back from YAADA and inspect:</p> <pre><code>avengers = list(context.query('Avenger'))\navengers[0]\n</code></pre> <p>should result in something like:</p> <p><pre><code>{'doc_type': 'Avenger',\n 'url': 'http://marvel.wikia.com/Henry_Pym_(Earth-616)',\n 'name_alias': 'Henry Jonathan \"Hank\" Pym',\n 'appearances': 1269,\n 'current_': 'YES',\n 'gender': 'MALE',\n 'full_reserve_avengers_intro': 'Sep-63',\n 'year': 1963,\n 'years_since_joining': 52,\n 'honorary': 'Full',\n 'death1': 'YES',\n 'return1': 'NO',\n 'notes': 'Merged with Ultron in Rage of Ultron Vol. 1. A funeral was held. ',\n 'id': '55f4787faa40baaf13540c4bf78509c976a5261050e400203355398209faa7ef',\n 'analytic_session_id': ['fcd6d744-375e-4b34-8fb2-fdcaebc21440'],\n 'analytic_name': ['IPython'],\n '@updated': '2022-08-22T21:40:12.600548',\n '@timestamp': '2022-08-22T21:40:12.600550',\n '_pipeline': [],\n '_id': '55f4787faa40baaf13540c4bf78509c976a5261050e400203355398209faa7ef'}\n</code></pre> Note that several fields (i.e. <code>analytic_name</code>, <code>analytic_session_id</code>, <code>@updated</code>, <code>@timestamp</code>, <code>_pipeline</code>, <code>_id</code>) were autogenerated and can safely be ignored. The most useful of those is <code>@timestamp</code> which tracks the time that the document was first ingested.</p>"},{"location":"tutorial/#defining-a-document-schema","title":"Defining a document schema","text":"<p>YAADA supports defining document schemas as json or yaml files following the OpenAPI 3.0 json-schema variant.</p> <p>To create a schema for our new <code>Avenger</code> document type, create a new file called <code>schema/Avenger.yaml</code> and put the following contents in it:</p> <p><pre><code>type: object\nproperties:\nid:\ndescription: The index-unique id used for writing into elasticsearch -- will be autogenerated if omitted.\ntype: string\ndoc_type:\ntype: string\nurl:\ntype: string\nname_alias:\ntype: string\nappearances:\ntype: integer\ncurrent_:\ntype: string\ngender:\ntype: string\nfull_reserve_avengers_intro:\ntype: string\nyear:\ntype: integer\nyears_since_joining:\ntype: integer\nhonorary:\ntype: string\ndeath1:\ntype: string\nreturn1:\ntype: string\nnotes:\ntype: string\ndeath2:\ntype: string\nreturn2:\ntype: string\nprobationary_introl:\ntype: string\ndeath3:\ntype: string\nreturn3:\ntype: string\ndeath4:\ntype: string\nreturn4:\ntype: string\ndeath5:\ntype: string\nreturn5:\ntype: string\nrequired:\n- doc_type\n- id\n- url\n</code></pre> Note that the filename (minus the <code>.yaml</code>) must exactly match the <code>doc_type</code> field in the data being ingested.</p> <p>After creating the schema file, you will need to exit your IPython session and relaunch, or restart your Jupyter kernel for the new schema to be picked up.</p> <p>Now try to ingest an invalid document and watch an exception get thrown:</p> <pre><code>context.update(dict(\n    doc_type=\"Avenger\",\n    id='foo'\n))\n</code></pre> <p>The above should result in a schema validation exception because we're missing the required <code>url</code> field.</p> <pre><code>ValidationError: 'url' is a required property\n\nFailed validating 'required' in schema:\n    {'definitions': {},\n     'links': {},\n     'nullable': False,\n     'properties': {'_id': {'description': 'The index-unique id used for '\n                                           'writing into elasticsearch -- '\n                                           'will be autogenerated if '\n                                           'omitted.',\n                            'type': 'string'},\n                    'appearances': {'type': 'integer'},\n                    'current_': {'type': 'string'},\n                    'death1': {'type': 'string'},\n                    'death2': {'type': 'string'},\n                    'death3': {'type': 'string'},\n                    'death4': {'type': 'string'},\n                    'death5': {'type': 'string'},\n                    'doc_type': {'type': 'string'},\n                    'full_reserve_avengers_intro': {'type': 'string'},\n                    'gender': {'type': 'string'},\n                    'honorary': {'type': 'string'},\n                    'id': {'description': 'The index-unique id used for '\n                                          'writing into elasticsearch -- '\n                                          'will be autogenerated if '\n                                          'omitted.',\n                           'type': 'string'},\n                    'name_alias': {'type': 'string'},\n                    'notes': {'type': 'string'},\n                    'probationary_introl': {'type': 'string'},\n                    'return1': {'type': 'string'},\n                    'return2': {'type': 'string'},\n                    'return3': {'type': 'string'},\n                    'return4': {'type': 'string'},\n                    'return5': {'type': 'string'},\n                    'url': {'type': 'string'},\n                    'year': {'type': 'integer'},\n                    'years_since_joining': {'type': 'integer'}},\n     'required': ['id', 'doc_type', 'url'],\n     'type': 'object'}\n\nOn instance:\n    {'@timestamp': datetime.datetime(2022, 8, 22, 22, 44, 41, 55061),\n     '@updated': datetime.datetime(2022, 8, 22, 22, 44, 41, 55056),\n     '_id': 'foo',\n     '_op_type': 'update',\n     '_pipeline': [],\n     'analytic_name': ['IPython'],\n     'analytic_session_id': ['84244b01-daaa-4392-a11e-ca7ffcd79494'],\n     'doc_type': 'Avenger',\n     'id': 'foo'}\n</code></pre>"},{"location":"tutorial/#querying-data","title":"Querying Data","text":"<p>This section will cover how to query data in YAADA. The methods that will be covered are <code>query</code>, <code>term_counts</code>, <code>exists</code>, <code>get</code>, and <code>rawquery</code> context methods. Content in this section assumes you have created an IPython shell with <code>yda run ipython</code> or using Jupyter Lab with <code>yda run jupyter</code> and have already constructed your YAADA <code>context</code>, and have followed the previous tutorial to ingest <code>Avenger</code> documents.</p>"},{"location":"tutorial/#fetch-documents-from-an-index-with-contextquery-method","title":"Fetch documents from an index with <code>context.query</code> method","text":"<p>This method uses the Elasticsearch scroll api through the Elasticsearch scan method and so can return extremely large numbers of documents if you are not careful.</p> <p>Basic usage of <code>query</code> to retrieve all documents in index just requires passing the <code>doc_type</code> as a parameter:</p> <p><pre><code>avengers = list(context.query('Avenger'))\nprint(len(avengers))\n</code></pre> Which should print:</p> <pre><code>173\n</code></pre> <p>Note that <code>context.query('Avenger')</code> returns a python generator that will lazily scroll through the Elasticsearch index, so we realize by wrapping with the <code>list</code> constructor.</p> <p>Now let's query for a subset of avengers using an Elasticsearch query. See Elasticsearch Query DSL documentation for more details.</p> <p>We are going to query for all avengers that have dies 3 times. Note that in the following query, we append a <code>.keyword</code> to the field we are querying on because by default, Elasticsearch maps string to a fuzzy search index mapping, and provides a <code>.keyword</code> variant for exact matching with <code>term</code> queries. <pre><code>avengers = list(context.query('Avenger',{\n    \"query\":{\n        \"term\": {\n            \"death3.keyword\":\"YES\"\n        }\n    }\n}))\nprint(len(avengers))\n</code></pre> Which should print:</p> <pre><code>2\n</code></pre> <p>To see the names of the two avengers returned, we can print with:</p> <pre><code>print([a['name_alias'] for a in avengers])\n</code></pre> <p>and get:</p> <pre><code>['Mar-Vell', 'Jocasta']\n</code></pre>"},{"location":"tutorial/#compute-value-counts-with-contextterm_counts-method","title":"Compute value counts with <code>context.term_counts</code> method","text":"<p>If we want to compute some high level summary statistics over some document field, we can use <code>context.term_counts</code>.</p> <p>To see the counts of MALE vs FEMALE avengers, we can use the following:</p> <p><pre><code>print(context.term_counts('Avenger','gender.keyword'))\n</code></pre> which should print: <pre><code>{'MALE': 115, 'FEMALE': 58}\n</code></pre></p>"},{"location":"tutorial/#check-if-a-document-exists-by-id-with-contextexists-method","title":"Check if a document exists by id with <code>context.exists</code> method","text":"<p>If we want to see if an <code>id</code> exists in a specific document index, we can use <code>context.get</code>.</p> <pre><code>print(context.exists('Avenger','e5d765e20f6e5e36f409a0cac8ff26bccc547a6316f4c5b5863ad360358cae89'))\n</code></pre> <p>should print:</p> <pre><code>True\n</code></pre>"},{"location":"tutorial/#fetch-specific-document-with-contextget-method","title":"Fetch specific document with <code>context.get</code> method","text":"<p>To fetch a document from an index by id, we can:</p> <pre><code>a = context.get('Avenger','e5d765e20f6e5e36f409a0cac8ff26bccc547a6316f4c5b5863ad360358cae89')\n</code></pre> <p>which when inspected should look like:</p> <pre><code>{'doc_type': 'Avenger',\n 'url': 'http://marvel.wikia.com/Vision_(Earth-616)',\n 'name_alias': 'Victor Shade (alias)',\n 'appearances': 1036,\n 'current_': 'YES',\n 'gender': 'MALE',\n 'full_reserve_avengers_intro': 'Nov-68',\n 'year': 1968,\n 'years_since_joining': 47,\n 'honorary': 'Full',\n 'death1': 'YES',\n 'return1': 'YES',\n 'notes': 'Dies in Avengers_Vol_1_500. Is eventually rebuilt. ',\n 'id': 'e5d765e20f6e5e36f409a0cac8ff26bccc547a6316f4c5b5863ad360358cae89',\n 'analytic_session_id': ['9d3e392b-f15e-4ac9-a24a-10b2c5145bf9'],\n 'analytic_name': ['IPython'],\n '@updated': '2022-08-22T22:40:01.920328',\n '@timestamp': '2022-08-22T22:40:01.920330',\n '_pipeline': [],\n '_id': 'e5d765e20f6e5e36f409a0cac8ff26bccc547a6316f4c5b5863ad360358cae89'}\n</code></pre> <p>If I don't want to see all those fields returned, I can choose the just return certain fields by passing a <code>source</code> argument.</p> <pre><code>a = context.get('Avenger','e5d765e20f6e5e36f409a0cac8ff26bccc547a6316f4c5b5863ad360358cae89',source=['doc_type','id','name_alias'])\n</code></pre> <p>which when inspected should look like:</p> <pre><code>{'name_alias': 'Victor Shade (alias)',\n 'doc_type': 'Avenger',\n 'id': 'e5d765e20f6e5e36f409a0cac8ff26bccc547a6316f4c5b5863ad360358cae89',\n '_id': 'e5d765e20f6e5e36f409a0cac8ff26bccc547a6316f4c5b5863ad360358cae89'}\n</code></pre>"},{"location":"tutorial/#raw-elasticsearch-queryaggregation-with-contextrawquery-method","title":"Raw Elasticsearch query/aggregation with <code>context.rawquery</code> method","text":"<p>The <code>context.query</code> method mentioned above used the Elasticsearch scroll API and only returns the source document, without any of the metadata from the Eleasticsearch query result envelope. If you want access to aggregations or query score data, you'll need to use <code>context.rawquery</code>.</p> <p>If I want to run an Elasticsearch aggregation to find out the range of <code>year</code> values in the <code>Avenger</code> index, I can run the following aggregation:</p> <pre><code>r = context.rawquery('Avenger',{\n    \"query\": {\n        \"match_all\": {}\n    },\n    \"aggs\": {\n      \"earliest_year\": {\n        \"min\": {\n          \"field\": \"year\"\n        }\n      },\n      \"latest_year\": {\n        \"max\": {\n          \"field\": \"year\"\n        }\n      }\n    },\n    \"size\": 0\n})\n</code></pre> <p>Inspecting the result should look like:</p> <pre><code>{'took': 3,\n 'timed_out': False,\n '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n 'hits': {'total': {'value': 173, 'relation': 'eq'},\n  'max_score': None,\n  'hits': []},\n 'aggregations': {'earliest_year': {'value': 1900.0},\n  'latest_year': {'value': 2015.0}}}\n</code></pre> <p>so we can see that the first year in the Avenger index is <code>1900.0</code> and the latest is <code>2015.0</code>.</p>"},{"location":"tutorial/#elasticsearch-schema-mapping","title":"Elasticsearch schema mapping","text":"<p>TODO</p>"},{"location":"tutorial/#applying-builtin-analyticspipelines","title":"Applying builtin analytics/pipelines","text":"<p>TODO</p>"},{"location":"tutorial/#writing-an-analytic","title":"Writing an Analytic","text":"<p>TODO</p>"},{"location":"tutorial/#writing-a-pipeline-processor","title":"Writing a Pipeline Processor","text":"<p>TODO</p>"},{"location":"tutorial/#extending-the-rest-api-with-custom-endpoints","title":"Extending the REST API with custom endpoints","text":"<p>TODO</p>"}]}